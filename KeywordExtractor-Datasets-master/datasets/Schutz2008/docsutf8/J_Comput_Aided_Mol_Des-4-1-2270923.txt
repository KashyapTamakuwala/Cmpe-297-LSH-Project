J Comput Aided Mol Des
Journal of Computer-Aided Molecular Design
0920-654X
1573-4951
Springer Netherlands
Dordrecht


2270923
18253702
9170
10.1007/s10822-008-9170-2
Article


What do we know and when do we know it?

Nicholls
Anthony

+1-505-4737385
anthony@eyesopen.com



OpenEye Scientific Software, Inc, 9D Bisbee Crt, Santa Fe, NM 87508 USA 

6
2
2008

3
2008

22
3-4
239
255
16
11
2007

2
1
2008


© The Author(s) 2008

Two essential aspects of virtual screening are considered: experimental design and performance metrics. In the design of any retrospective virtual screen, choices have to be made as to the purpose of the exercise. Is the goal to compare methods? Is the interest in a particular type of target or all targets? Are we simulating a ‘real-world’ setting, or teasing out distinguishing features of a method? What are the confidence limits for the results? What should be reported in a publication? In particular, what criteria should be used to decide between different performance metrics? Comparing the field of molecular modeling to other endeavors, such as medical statistics, criminology, or computer hardware evaluation indicates some clear directions. Taken together these suggest the modeling field has a long way to go to provide effective assessment of its approaches, either to itself or to a broader audience, but that there are no technical reasons why progress cannot be made.

Keywords
Virtual screening
Statistics
AUC
ROC curves
Metrics

issue-copyright-statement
© Springer Science+Business Media B.V. 2008




Introduction
Virtual screening in the pharmaceutical industry is an essential part of molecular modeling’s contribution to lead discovery and, to a lesser extent, lead optimization. This has led to considerable research into what method or approach works best, typically by means of ‘retrospective’ evaluations, i.e. attempting to predict future, i.e. prospective, behavior by appraising techniques on known systems. Despite this there is no agreed upon theory as to how to conduct a retrospective evaluation. As a consequence, it is very difficult for an outsider to assess if methods are getting better, have stayed the same, or even worsened over time. In a practical enterprise, such as drug discovery, the proposed benefits of virtual screening, i.e. avoiding the cost and time of a real screen, have to be weighed against one simple question: does it actually work? Without proper metrics of success, i.e. ones that go beyond the anecdotal, molecular modeling is not guaranteed a vibrant future.
1
2
]. In reports on virtual screening, in fact in molecular modeling in general, it is rare to find an adequate consideration of any of these issues.
Why is this? Why is the modeling field so poor at the most basic elements of evaluation? A charitable view would be that, as with communication skills, most modelers receive little appropriate formal training. Certainly there is no central resource, whether scholastic review, book or paper. A slightly less charitable view is that journals have not developed standards for publication and as such there is little Darwinian pressure to improve what the community sees as acceptable. It is to be hoped that this is a learning curve, i.e. that editors will eventually appreciate what is required in a study. An extreme view is that we are poor at evaluations because we simply do not matter very much. If large fortunes were won or lost on the results from computational techniques there would be immense debate as to how to analyze and compare methods, on what we know and exactly when we know it. There would be double blind, prospective and rigorously reviewed studies of a scale and depth unknown in our field but common in, for instance, clinical trials. In short, there would be standards.
variance
3
5
6
9
]. Consideration of why the AUC is a popular measure in many disciplines suggests standards by which virtual screening metrics ought to be judged. Finally, by evaluating average properties of large numbers of systems, and by considering simple cost/benefit examples, we bring into question the validity and utility of metrics proposed to capture ‘early’ behavior.

Experimental design
In what follows we consider the importance of both intensive and extensive properties of an experiment. An intensive property is something intrinsic to a design, whereas extensive properties change with the size of the system. For example, the type of decoys used in a retrospective study is an intensive property; the number of such is an extensive property. We believe the most overlooked intensive characteristic is the design goal, i.e. what is trying to be proved. This typically falls into a few discrete classes and appropriate labeling would help combine lessons from different studies. For extensive quantities we consider how common statistical approaches can aid the choice of numbers of actives, decoys and targets. Finally, actives, decoys, targets or methods are not always independent and this has to be quantified even in as simple a matter as comparing two programs. Techniques for accounting for correlation within an experimental design are known but rarely applied.
Intensive properties
One of the most basic issues in designing a retrospective screen is how to chose decoys. Typically there are a certain number of active compounds and one wishes to see if a method can distinguish these from a second set, presumed inactive. This is the most basic of classification problems. Is X of type A or type B? The legal system often has the same dilemma, e.g. was X at the scene of a crime or not? A police line-up has all the components of a virtual screen. Usually the number of actives (suspects) is small, usually one. The number of decoys (called ‘fillers’) has to be sufficient that random selection does not compete with real recognition; a minimum of four is usual. But it cannot be so large that guilt is hidden within the statistical variance of the innocent. The fillers need to be convincing, i.e. not outlandishly dissimilar to the guilty party, but not too similar or even potentially also at the scene (i.e. false false positives). As courtroom verdicts can depend on the appropriateness of a line-up, standard procedures are well known.
Universal
. Any compound available to be physically screened, typically either from vendors or corporate collections.

Drug-like
. Available and drug-like, typically by applying simple chemical filters.

Mimetics
known
 ligands by simple physical properties.

Modeled
. Available, drug-like and derived using 3D modeling on known ligands or the intended targets.


Although no classification scheme could be perfect, fair comparison of studies requires an alignment of intent. In general, decoys get ‘harder’ from A to D, although this is not necessarily true on a case-by-case basis and is itself an interesting area of research.
universal
universal
10
prior
3
universal
11
12
12
13
14
3 
15
3
4
universal
 decoys; in fact such decoys may prove difficult for some computation methods, the Hawkins dog test not with-standing. The point is that a presumed limitation can be overcome by applying basic error analysis.
16
universal
drug-like
 decoys. Some companies’ collections are heavily biased towards certain targets that may or may not be related to the retrospective study at hand. The study by McGaughey et al. reported significant differences in the efficiency of decoys chosen from the MMDR, a kind of ‘consensus’ drug-like collection, compared to ones from their internal Merck database. This trade of generality for local applicability is a characteristic of many aspects of evaluations. For instance, should targets be chosen to represent all possible systems, a subset of pharmaceutical interest or a class within that subset? What is gained in local applicability is often lost to generality and prospective predictability.
mimetic
modeled
Mimetic
17
18
mimetic
self
drug-like
all
Mimetic
self
all
mimetics
 are not guaranteed to provide a reality check for methods claiming to capture the physics of drug-target interaction.
Modeled
mimetics
seen
modeled
mimetic
modeled
 decoys, i.e. the aim was to make things harder for docking programs, but, as mentioned, this was not always achieved.
mimetic
modeled
mimetic
modeled
 decoys the procedure applied must be scrupulously described, e.g. how is the protein prepared, how is the ligand protonated etc, and complete and accurate descriptions of published virtual screening procedures are rare. However, there seems no reason a consensus could not be reached by interested parties. Standard protocols could be developed, shared and used to verify results. The problems are more of will than ingenuity.
universal
drug-like
mimetic
modeled
19
all
 decoy types, with careful labeling of individual intent. The authors can make of their data what they will, for instance by reporting performance against a subset of decoys. However, if a broader set is included in the supplementary material, others can make use of the data for potentially different purposes. One of the proposals of this paper is for modeling to move beyond the anecdotal towards the systematic. Full reporting of data is essential but a further step would be to include alternate data so that others can construct purposes beyond the original intent.

Extensive properties
In addition to intensive properties, there are the extensive properties such as how many actives, decoys and targets are used. Once again the important consideration is knowing what we want to know. If the purpose is to evaluate a single method on a single target the necessary extensive properties are quite different than for a broad study on the efficacy of several methods on many targets. We illustrate this with some basic error analysis.
M
V
M
V
V
M
3
5
].
1
1
4
21
Fig. 1
a
b
20
]. Also included for comparison purposes is the average AUC for GOLD against the Warren set with associated error bars




independent
 sources of error, the expected error is formed from the root mean square of the individual sources of error, thus:
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Err}} \approx {\sqrt {{\left( {{\text{Err}}_{{\text{1}}} ^{{\text{2}}} {\text{ + Err}}_{{\text{2}}} ^{{\text{2}}} {\text{ + Err}}_{{\text{3}}} ^{{\text{2}}} {\text{ \ldots }}} \right)}} } $$\end{document}

 For our case we can write:
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ \begin{aligned}{} {\text{Err(method)}} & \approx {\sqrt {{\text{(Err}}_{{{\text{targets}}}} ^{{\text{2}}} {\text{ + Err}}_{{{\text{actives}}}} ^{{\text{2}}} {\text{ + Err}}_{{{\text{inactives}}}} ^{{\text{2}}} {\text{)}}} } \\ & \approx {\sqrt {{\text{(Var}}_{{{\text{targets}}}} {\text{/N}}_{{\text{t}}} {\text{ + Var}}_{{{\text{actives}}}} {\text{/N}}_{{\text{a}}} {\text{ + Var}}_{{{\text{inactives}}}} {\text{/N}}_{{\text{i}}} {\text{)}}} }{\text{ }} \\ \end{aligned} $$\end{document}


The variances are intrinsic properties to ‘targets’, ‘actives’ and ‘inactives’. How do we know what these variances are? One way is to boot-strap, i.e. leave out a randomly chosen fraction of the targets, or subset of actives or inactives, and measure changes in performance. Repeating this procedure many times gives a statistical sampling of the sensitivity to outliers and the number of measurements. Alternatively, in some cases the variance can be established more precisely. In the case of AUC, for example, it can be shown that for a particular target the variance for both actives and inactives can be approximated by:
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Var}}_{{{\text{active}}}} {\text{ = }}\sum {\text{ (p}}_{{\text{i}}} - \langle {\text{p}}\rangle {\text{)}}^{{\text{2}}} {\text{/N}}_{{{\text{active}}}} {\text{ }} $$\end{document}


\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Var}}_{{{\text{active}}}} {\text{ = }}\sum {\text{ (q}}_{{\text{j}}} - \langle {\text{q}}\rangle {\text{)}}^{{\text{2}}} {\text{/N}}_{{{\text{inactive}}}} {\text{ }} $$\end{document}

i
i
j
j
7
2
Fig. 2
AUC values ordered from left to right by number of actives for each target in the DUD set. Program used: FRED with Chemscore as the posing and scoring function. Error bars are 95% confidence intervals for each virtual screen




\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Err(AUC) }} \approx {\sqrt {{\text{(Var}}_{{{\text{Obs}}}} {\text{/N}}_{{\text{t}}} {\text{)}}} } $$\end{document}

where
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Var}}_{{{\text{Obs}}}} {\text{ = }}\sum {\text{ (AUC}}_{{\text{i}}} - \langle {\text{AUC}}\rangle {\text{)}}^{{\text{2}}} {\text{/N}}_{{\text{t}}} $$\end{document}


Therefore
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Var}}_{{{\text{targets}}}} {\text{ = N}}_{{\text{t}}} {\text{ * \{(Var}}_{{{\text{Obs}}}} {\text{/N}}_{{\text{t}}} {\text{)}} - {\text{(Var}}_{{{\text{actives}}}} {\text{/N}}_{{\text{a}}} {\text{)}} - {\text{(Var}}_{{{\text{decoys}}}} {\text{/N}}_{{\text{i}}} {\text{)\} }} $$\end{document}


1
self
self
system
method
self
Table 1
The contribution to observed variance from actives, decoys and targets over the DUD dataset (DUD-self decoys)

Method
2
〉 − Decoys
2
〉 − Actives
2
〉 − Observed
2
〉 − Targets


FRED
0.000048
0.0020
0.023
0.021

ROCS
0.000025
0.0022
0.041
0.039

MACCS 
0.00004
0.0017
0.030
0.028

LINGOS
0.000039
0.0017
0.035
0.033



The estimated error (squared) from the variation between targets is estimated from the observed variance and corresponds to that which would be obtained if the number of actives and inactives were infinite



When calculating the properties of a single system the number of actives is fairly important, but the number of inactives does not have to be substantially larger. A ratio of decoys to actives of 4:1 only has an error 11% higher than the limiting value from an infinite number of inactives. It would be more useful to include sets of inactives designed for different purposes than to attempt to ‘overwhelm’ the actives with decoys.

If the purpose is to test a method against other methods with 95% confidence then the number of systems required is very large, much larger than even DUD. In our analysis the contributions to the variance from a limited numbers of actives is almost insignificant compared to the target-to-target variation. For example, it would take over 100 test systems to tease apart the difference between the ligand-based method ROCS and the docking program FRED with 95% confidence. (See below.)

22
], may be statistically quite valid.




Correlations
i
independent
22
23
operationally
operational
 part of this definition depends on the nature of the method, i.e. dependence is conditional on the nature of the procedure investigated.
24
universal
drug-like
mimetic
modeled
not as decoys
latent
latent
explicit
drug-like
 decoys, but it fortuitously provides the most compact form for a rigorous estimation of decoy/active independence. This work will be presented elsewhere, along with an elaboration of the techniques for assessing operational correlation.
3
independent
Fig. 3
Docking performance against the two isoforms in the Warren study (PDFS and PDFE), compared to the averaged difference over all other pairs of targets




new
 target and so targets are chosen to be diverse. Software would seldom be used in default, out-of-the-box, mode when there is considerable domain knowledge, i.e. within a set of closely related targets. Hence, the question of method variance over similar systems appears to have been over-looked.
1
differences
 between methods, e.g. for an AUC example the variance becomes:
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Var}}_{{{\text{diff}}}} {\text{ = }} \sum{\text {((}}{\text{A}}_{{\text{i}}}  -  {\text{B}}_{{\text{i}}} {\text{)}} - {\text{(}}\langle {\text{A}}\rangle - \langle  {\text{B}}\rangle {\text{))}}^{{\text{2}}} {\text{/N}}_{{\text{t}}} $$\end{document}


\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Err(diff) }} \approx {\sqrt {{\text{(Var}}_{{{\text{diff}}}} {\text{/N}}_{{\text{t}}} {\text{)}}} } $$\end{document}


diff
 can be rewritten as:
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Var}}_{{{\text{diff}}}} {\text{ = Var}}_{{\text{A}}} {\text{ + Var}}_{{\text{B}}} - {\text{2*Corr(A,B) }} $$\end{document}


\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Corr(A,B) = }}\sum {\text{(}}{\text{A}}_{{\text{i}}} - \langle {\text{A}}\rangle {\text{)}}{\text{(B}}_{{\text{i}}} - \langle {\text{B}}\rangle {\text{)/N}}_{{\text{t}}} $$\end{document}


Here Corr(A,B) is a measure of the correlation between methods A and B and is related to the Pearson correlation coefficient, thus:
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Pear(A,B) = Corr(A,B)/}}{\sqrt {{\text{( Var}}_{{\text{A}}} {\text{*\,Var}}_{{\text{B}}} {\text{)}}} } $$\end{document}


joint
 error bar √2 larger than the individual error bars. (This also means the common practice of evaluating whether two methods are statistically different by whether their individual error bars overlap is generally incorrect.)
difference
 is zero. In general, methods tend to be positively correlated so that the joint confidence limits are lower than from independent measurements.
p
p
p
-)value to the probability one method is better only because of random chance. We do this by calculating the area under the normal form for which one appears better than the other. The mathematics of this is shown below:
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ p{\text{ = (1}}-erf{\text{(}}\langle {\text{A}}-{\text{B}}\rangle {\text{*}}{\surd{{\text{(0}}{\text{.5*N}}_{{\text{t}}}{\text{/Var}}_{{{\text{diff}}}}))) {\text{/2}}} }{\text{ }} $$\end{document}

erf
error
t
p
estimate
p
-value refers to the dichotomous question, is A better than B?
2
1
self
p
14
18
22
9
Table 2
Statistical measures necessary to accurately assess the relative performance of methods, here applied to the DUD data set (DUD-self decoys)

Method
FRED
ROCS
MACCS
LINGOS


FRED
0.684/0.043
0.11/0.08/0.07
0.1/0.07/0.06
0.1/0.07/0.065

ROCS
0.17/0.09
0.732/0.065
0.12/0.085/0.05
0.125/0.09/0.05

MACCS 
0.03/0.05
0.70/0.47
0.734/0.055
0.115/0.08/0.055

LINGOS
0.19/0.14
0.65/0.36
0.54/0.31
0.72/0.061



p
-values that a method has a higher mean AUC by random chance



p
25
p
5
].
2
] and to our knowledge has not been properly applied to comparing methods in virtual screening.


Metrics
necessarily
 to be prized. By looking at a large number of virtual screens, we will ask whether such ‘early’ measures are necessary and whether they can be predicted from more fundamental and well-understood properties. Finally, the application of accurate error bounds will be shown to suggest at least one way of quantifying the advantage an expert brings to well-studied systems.
Properties of virtual screening metrics
26
]. SPEC Marks have evolved over time to now cover CPU, graphics, Java, mail servers, file servers, parallel performance, high performance computing and other aspects. In other words, SPEC is not a single measure because not all users want the same thing, but this does not mean manufacturers can create their own metrics. Rather SPEC is an umbrella organization for a set of open and diverse groups that consider, ratify and develop benchmarks. In this spirit, this section will concentrate on what ought to be general characteristics of a good metric rather than all prevalent quantities.
In a somewhat circular manner, one of the first characteristics of a good measure is that everyone uses it. Clearly one of the problems with a field with diverse measures is incomparability, the “apples and oranges” problem. The most straightforward solution is not imposition of a particular standard but full disclosure of all data. The authors of a study may want to present enrichment at 5%, but if the data is freely available others may calculate the enrichment at 1% or 13% or whatever they wish. This would inevitably lead to standardization as independent parties harvest data from many sources, publishing larger and larger studies on the advantages and disadvantages of different methods and measures. This would provide another example of meta-analysis described above. Sometimes a valid excuse against disclosure is that compounds or targets are proprietary. However, just providing lists of actives and inactives in rank order with unique, but not necessarily identifying, tags is enough to calculate most of the metrics for a particular virtual screen. Currently the field of modeling lacks even an agreed upon format for the exchange of such rarely available information.
Independence to extensive variables

Robustness

Straightforward assessment of error bounds

No free parameters

Easily understood and interpretable



Take for example the very popular “enrichment” measure. Everyone understands the concept of enrichment: swirl a pan of water and gravel from the Klondike river in 1896 in just the right way and you ended up with mostly gold. In virtual screening you look at the top few percent and see whether there are more actives than you would expect by chance. As a mathematical formula this is typically presented as:
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{EF(X\%) = (100/X) * (Fraction of Actives Found)}} $$\end{document}


improvement
particular
 experiment. If the ratio of inactives to actives becomes very large it is assumed this problem disappears, i.e. that the limiting behavior obeys (i). This is not true if the enrichment at a given percent is large, i.e. at precisely the points of most interest. Also, enrichment does not meet requirements (ii). At a small enough percentage the enrichment becomes an unstable function of the exact positions of actives in a list. There is also no agreed upon percentage, making this an adjustable parameter (often abused). Finally, other than by bootstrapping, the author knows of no simple assessment of error bounds. However, it is an intuitive measure, easily understood, passing rule (v), and so almost uniquely to this field is the most common metric reported.
27
14
] have used this alternate form. Perhaps the only important failing of this measure is that it lacks a specific name. For the purpose of this paper it will be referred to as the ROC enrichment to distinguish it from the widely abused variety.
6
9
] and it has become a standard for classification performance in many disciplines (medical diagnostics, radiology, clinical testing, criminology, machine learning, data mining, psychology and economics to name a few). It satisfies all of the criteria listed above as a metric, including (v), ease of interpretation. The AUC is simply the probability that a randomly chosen active has a higher score than a randomly chosen inactive. The main complaint against the AUC is that is does not directly answer the questions some want posed, i.e. the performance of a method in the top few percent. This is akin to complaining that SPEC Marks do not do a good job of evaluating mobile phone processors; a fair complaint perhaps but hardly justifying creating a new benchmark without the strengths of existing standards. The AUC ought to at least be held as such a standard against which new measures are judged.

Early performance in virtual screening
4
Fig. 4
Example ROC plots for “early” and “late” methods




28
29
4
, the ratio of the BedROC score of the solid line to the dashed is about two for a beta of ten and about ten for a beta of twenty.
So does BedROC or RIE qualify as a good metric for virtual screening? Comparing against the five criteria listed above, both are more robust than enrichment, and the error protocols for BedROC satisfies criteria (iii). RIE suffers from having an ill-defined numerical interpretation (i.e. how good is an RIE of 5.34?). BedROC attempts to overcome this by scaling between 0.0 and 1.0, but does this qualify as being understandable? There is no absolute, interpretable meaning to a BedROC (or RIE) number, only a relative meaning when ranking methods.
4
, a factor of two in BedROC between the methods (beta = 5.0) does not sound anywhere near as bad as a factor of ten (beta = 20.0).

Cost structures of virtual screening
4
any
30
]. What are not made explicit in this shift are the costs of the four components of any virtual screen: true positives (TP), false negatives (FN), false positives (FP) and true negatives (TN). Not wanting to miss anything is equivalent to assigning an infinite cost to a false negative. This was never sensible, but reflected a ‘lottery’ mentality prevalent at the time. The reality is that virtual screening never finds drugs; at best it can find things that might, after considerable effort, become drugs. In addition, the attrition rate at many stages in the drug design process means any lead-like compound is at best a bet that will often fail, costing many millions of dollars. A lottery ticket is potentially worth millions; the expected value, i.e. averaged over all contingencies, is usually less than the cost of the ticket. The assumption behind virtual screening is that the value of a true positive similarly averaged is worth the cost of computers and modelers. This is an unproven conjecture.
rates
4
TP = 8.0

FN = −2.0

FP = −0.16

TN = 0.02



t
 depends on the False Positive Rate (FPR) and True Positive Rare (TPR):
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Cost(t) = TPR\,*\,N}}_{{\text{a}}}{\text{\,*\,(8}}{\text{.0) + (1}} - {\text{TPR)\,*\,N}}_{{\text{a}}}{\text{\,*\,(}}-{\text{2}}{\text{.0) + FPR\,*\,N}}_{{\text{i}}}{\text{\,*\,(}}-{\text{0}}{\text{.16) +(1}}-{\text{FPR)\,*\,N}}_{{\text{i}}} {\text{\,*\,(0}}{\text{.02)}}  $$\end{document}


a
i
 = 1/100, then:
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ \begin{aligned}{} {\text{ Cost(t)/Ni }} {\text{= (TPR\,*\,(8}}.0 +2.0)-{\text{2}}{\text{.0)/100}} -{\text{FPR\,*\,(0}}{\text{.16 + 0}}{\text{.02) + 0}}{\text{.02}}\\ {\text{ = 0}}{\text{.10\,*\,TPR}} -{\text{0}}{\text{.18\,*\,FPR}}\\ \end{aligned} $$\end{document}


4
5
Fig. 5
a
b
) Cost weighted versions of the curves in Fig. 4 as per the second description in the text




TP = 8.0

FN = −2.0

FP = −0.04

TN = 0.03



\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{Cost(t)/N}}_{{\text{i}}} {\text{ =0}}{\text{.1\,*\,TPR}}-{\text{0}}{\text{.07\,*\,FPR + 0}}{\text{.01}} $$\end{document}


5
b illustrates the effect of these new weightings. By reducing the cost of a false positive by 75%, i.e. to around the savings of a true negative, both methods are always cost effective. Furthermore, although the early method has a clear maximum at around 20% of the database, it is actually worth physically screening about 75% of the database.
These examples are obviously only illustrative, but the point they make is real. Early enrichment is important only because of an assumed cost structure. Clearly much more complicated models could be constructed, possibly with real data, as with medical tests. However, to the author’s knowledge this has never been published, presented or even discussed within the industry. It is an assumption that early enrichment is better. Likewise, it is also an assumption that virtual screening itself is a productive exercise compared to physical screening.

Averaged properties of virtual screening
4
6
4
target
averaged
7
1
7
8
9
Fig. 6
Averaged ROC curves for twenty methods in the Warren study for which scores for all eight targets where available. Programs and scoring functions listed to the right of the graph



Fig. 7
Average ROC curves for FRED, ROCS, MACCS keys and LINGOS over DUD, with DUD-self decoys. FRED was run with the ChemGauss3 scoring function



Fig. 8
BedROC scores with an exponential factor of 5.0 versus the AUC for 270 virtual screens from the Warren study



Fig. 9
The average AUC for each method run against all eight targets in the Warren study versus the averaged BedROC score for each such method




9
. This point lies outside the 95% confidence limits of both AUC and BedROC. Its BedROC score is significantly higher than expected and its AUC is around 0.5, i.e. random ranking. The target protein represented by this point is PPAR-δ and the method is MVP, a program developed in-house at GSK by Mill Lambert. In conversation Lambert freely admitted that not only did he have extensive knowledge of this target, he used all of this information to tune MVP. Unfortunately, because of certain aspects of the target he could only select one of three chemical classes for this ‘hands-on’ treatment at a cost to the two other classes. Hence, MVP had to be biphasic. It seems interesting that out of two hundred and seventy virtual screens the only outlier from the BedROC-AUC correspondence is an example of expert intervention. An unintended consequence of this study might be a method to spot and quantify expert contributions to virtual screening, i.e. by comparing early behavior, either with BedROC or other metrics, to that predicted from the fundamental measure of AUC.


Conclusions
In this study we have considered several aspects of experimental design and performance metrics for virtual screening. There is clearly interest in doing things the right way, not least because of a popular, if unproven, belief that virtual screening saves the pharmaceutical industry money. As with many relatively young endeavors, molecular modeling has been long on promises and short on standards, and it is standards that ultimately deliver the proof that our field is useful. For many years the computer industry suffered from similar growing pains. Not only were there few, if any, reliable comparison metrics for different processors, operating systems, compilers and so forth, the proposed benefits of computers were more assumed than quantified. These days no one doubts the impact of the computing revolution. It is to be hoped that a similar statement can one day be made for molecule modeling. It is with this in mind that the following observations and recommendations are made.
universal
drug-like
mimetic
modeled
 based on examples from the literature and on typical use-case analysis.

Providing access to primary data would allow the field to gain cumulative knowledge. The field of modeling has almost no “meta-analysis”, i.e. research combining the results from studies, largely because of a lack of standards as to procedures and measures, but also due to the lack of primary data. A comprehensive format for virtual screening information would be useful.

The inclusion of multiple decoy sets of different design and intent for each target in an evaluation would, in combination with (i) and (ii) above, greatly increase the cumulative value of published studies.

The number of targets, actives and inactives need to be carefully considered with respect to the purpose of the experiment and the required accuracy of the results. These can be derived from simple statistical methods that are almost never applied.

explicit
latent
 decoys for all other targets. Warren et al. provides an example of the first, i.e. decoy sets were made from the actives of other targets. The second is an extension of point (iii), i.e. include multiple sets of decoys in a study but for different purposes. In conjunction with (ii) above, this would provide material for a rigorous analysis of operational correlation in virtual screening.

Correlation between targets needs further research, in particular the question of the variance of computational methods on closely related systems.

p
-values for any assessment of method superiority.


Deciding on the metrics to be reported should be a community effort, although access to primary data to encourage “meta-analysis” would aid the autonomous adoption of metrics.

There are good reasons metrics such as the AUC are popular in other fields and any new or additional measures for virtual screening need to be assessed against the characteristics that have made such metrics successful. Five characteristics required for a metric to be of similar heft to the AUC are proposed: independence to extensive variables, robustness, error bounds, no adjustable parameters and ease of interpretation. As an illustration, an improvement to the common enrichment measure is described. We propose the term “ROC enrichment” for this new measure. Similar improvements to early measures are proposed.

average
 behavior of interest.

The assumption that ‘early’ behavior is necessarily a benefit is based on an assumed cost structure that may or may not hold. Similar statements are true for virtual screening in general. A rigorous attempt to assign real-word costs would be of use to the field.

average
 behaviors cannot be accurately predicted by AUC or obvious extensions there of. Those suggesting otherwise need to provide clear-cut, statistically valid, evidence.

Divergence from (v) may be an indicator of local or domain knowledge, i.e. knowing the right answer and/or extensive knowledge of the system under study. A potential future area of research is whether this is also an indicator of over-parameterization, posterior system preparation or other reliance on retrospective knowledge. Interestingly, 2D methods applied to DUD, showed no evidence of such a divergence.


In conclusion, there is no reason it is not possible to establish standards in the field of molecular modeling necessary to enhance the quality of publications and allow a reliable assessment of methods and progress. However, there are also powerful incentives not to be rigorous. As one invested scientist was heard to pronounce, “livelihoods are at stake”. This is true; we suggest the livelihood of the entire field. Whether the modeling community has the will to enact such measures may well determine whether future generations of scientists look back and see a field that became essential to drug discovery or one that became a mere footnote.


Acknowledgements
The author wishes to thank Ajay Jain for discussions and his efforts on the “Evaluation of Computational Methods” symposium at the 234th American Chemical Society meeting, Martha Head for sharing the AUC and rank orderings of the GSK docking study, and Geoff Skillman, Paul Hawkins and Mark McGann for ideas, discussions and, most importantly, data. Finally, Christopher Bayly and Jean-Francois Truchon for spirited discussions on BedROC that, even if they did not convince the author, provided the spur for much of the work presented here.
Open Access
 This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.

References
1.
Ioannidis
JPA


Contradicted and initially stronger effects in highly cited clinical research
JAMA
2007
294
2
218
228
10.1001/jama.294.2.218

16014596


2.
Obuchowski
NA

Lieber
ML

Wians
FH


Clinical Chemistry
: Uses, misuses and possible solutions
Clin Chem
2004
50
7
1118
1125
10.1373/clinchem.2004.031823

15142978


3.
Warren
GL

Andrews
CW

Capelli
AM

Clarke
B

LaLonde
J

Lambert
MH

Lindvall
M

Nevins
N

Semus
SF

Senger
S

Tedesco
G

Wall
ID

Woolven
JM

Peishoff
CE

Head
MS


A critical assessment of docking programs and scoring functions
J Med Chem
2006
49
20
5912
5931
10.1021/jm050362n

17004707


4.
McGaughey
GB

Sheridan
RP

Bayly
CI

Culberson
JC

Kreatsoulas
C

Lindsley
S

Maiorov
V

Truchon
J-F

Cornell
WD


Comparison of topological, shape, and docking methods in virtual screening
J Chem Inf Model
2007
47
4
1504
1519
10.1021/ci700052x

17591764


5.
Sheridan RP This issue

6.
Hanley
JA

McNeil
BJ


The meaning and use of the area under an ROC curve
Radiology
1982
143
29
36

7063747


7.
DeLong
ER

DeLong
DM

Clarke-Pearson
DL


Comparing the areas under two or more correlated receiver operating characteristic curves: A nonparametric approach
Biometrics
1988
44
837
845
10.2307/2531595

3203132


8.
Metz
CE


Basic principles of ROC analysis
Semin Nuclear Med
1978
VIII
4
283
298
10.1016/S0001-2998(78)80014-2

Metz CE (1978) Basic principles of ROC analysis. Semin Nuclear Med VIII(4):283–298 

9.
Hanley
JA

McNeil
BJ


A method of comparing the areas under the receiver operator characteristic curves derived from the same cases
Radiology
1983
148
839
843

6878708


10.
Bissantz
C

Folkers
G

Rognan
D


Protein-based virtual screening of chemical databases. 1. Evaluations of diffferent docking/scoring combinations
J Med Chem
2000
43
25
4759
4767
10.1021/jm001044l

11123984


11.
Weininger D (1998) Combinatorics of small molecular structures. Encyclopedia of computational chemistry, vol 1. Wiley, New York, pp 425–430

12.
Fink
T

Bruggesser
H

Reymond
J-L


Virtual exploration of the small molecule chemical universe below 160 Daltons
Angew Chem Int Ed
2005
44
1504
1508
10.1002/anie.200462457

Fink T, Bruggesser H, Reymond J-L (2005) Virtual exploration of the small molecule chemical universe below 160 Daltons. Angew Chem Int Ed 44:1504–1508 

13.
Jain
AN


Ligand-based structure hypothesis for virtual screeninG
J Med Chem
2004
47
947
961
10.1021/jm030520f

14761196


14.
Cleves AE, Jain AN This issue

15.
Hawkins P (2007) On how not to do an evaluation. Presentation at CUP8, Santa Fe, New Mexico, February 26th–28th 2007

16.
Lipinski CA, Lombardo F, Dominy BW, Feeney PJ (2001) Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. Adv Drug Del Rev 46:3–26

17.
Verdonk
ML

Berdini
V

Hartshorn
MJ

Mooij
WTM

Murray
CW

Taylor
RD

Watson
P


Virtual screening using protein-ligand docking: Avoiding artificial enrichment
J Chem Inf Comput Sci
2004
44
793
806
10.1021/ci034289q

15154744


18.
Huang
N

Shoichet
BK

Irwin
JJ


Benchmarking sets for molecular docking
J Med Chem
2006
49
23
6789
6807
10.1021/jm0608356

17154509


19.
Geoffrey Skillman A (2007) Personal communication

20.
ROCS, FRED are products of OpenEye Scientific Software, www.eyesopen.com. LINGOS are based on the OEChem SMILES canonicalization, also from OpenEye. MACCS Keys are fingerprints of molecular 2D features

21.
Hawkins
PC

Skillman
AG

Nicholls
A


Comparison of shape-matching and docking as virtual screening tools
J Med Chem
2007
50
1
74
82
10.1021/jm0603365

17201411


22.
Good A, Oprea T This issue

23.
Clark RD This issue

24.
Cheverud
JM


A simple correction for the multiple comparisons in interval mapping genome scans
Heredity
2001
87
52
58
10.1046/j.1365-2540.2001.00901.x

11678987


25.
Fisher RA (1948) Combining independent tests of significance. Am Stat 2(5):30

26.
www.spec.org/spec


27.
Pepe MS (2003) The statistical evaluation of medical tests for classification and prediction. Oxford Statistical Science Series. Oxford University Press. Chapter 5: Empircal Estimation

28.
Sheridan
RP

Singh
SB

Fluder
EM

Kearsley
SK


Protocols for bridging the peptide to nonpeptide gap in topological similarity searches
J Chem Inf Comput Sci
2001
41
5
1395
1406
10.1021/ci0100144

11604041


29.
Truchon
J-F

Bayly
CI


Evaluating virtual screening methods: good and bad metrics for the “early recognition” problem
J Chem Inf Model
2007
47
2
488
508
10.1021/ci600426e

17288412


30.
Bayly CI, Brideau C, Liaw A, Svetnick V (2006) Iterative focused screening using Random Forest: A comparison with HTS/random screening for two extreme cases. Thomas Kuhn Paradigm Shift Award Competition, 231st ACS National Meeting. March 26–30, 2006




