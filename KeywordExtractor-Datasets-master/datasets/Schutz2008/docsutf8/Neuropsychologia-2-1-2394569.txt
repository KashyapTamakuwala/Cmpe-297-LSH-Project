Neuropsychologia
Neuropsychologia
0028-3932
Pergamon Press


2394569
18249420
NSY2778
10.1016/j.neuropsychologia.2007.11.026
Article


Cortical circuits for silent speechreading in deaf and hearing people

Capek
Cheryl M.

c.capek@ucl.ac.uk
a
⁎

MacSweeney
Mairéad

b

Woll
Bencie

a

Waters
Dafydd

a
b

McGuire
Philip K.

c

David
Anthony S.

c

Brammer
Michael J.

c

Campbell
Ruth

a


a
Deafness, Cognition and Language Research Centre, Division of Psychology and Language Sciences, University College London, 49 Gordon Square, London WC1H 0PD, United Kingdom

b
Behavioural and Brain Sciences Unit, UCL Institute of Child Health, University College London, 30 Guilford Street, London WC1N 1EH, United Kingdom

c
Institute of Psychiatry, Kings College London, De Crespigny Park, London SE5 8AF, United Kingdom

⁎
c.capek@ucl.ac.uk


2008

46
5
1233
1241
1
3
2007

8
10
2007

26
11
2007


© 2008 Elsevier Ltd.
2007
Elsevier Ltd
certain conditions
.


Abstract
This fMRI study explored the functional neural organisation of seen speech in congenitally deaf native signers and hearing non-signers. Both groups showed extensive activation in perisylvian regions for speechreading words compared to viewing the model at rest. In contrast to earlier findings, activation in left middle and posterior portions of superior temporal cortex, including regions within the lateral sulcus and the superior and middle temporal gyri, was greater for deaf than hearing participants. This activation pattern survived covarying for speechreading skill, which was better in deaf than hearing participants. Furthermore, correlational analysis showed that regions of activation related to speechreading skill varied with the hearing status of the observers. Deaf participants showed a positive correlation between speechreading skill and activation in the middle/posterior superior temporal cortex. In hearing participants, however, more posterior and inferior temporal activation (including fusiform and lingual gyri) was positively correlated with speechreading skill. Together, these findings indicate that activation in the left superior temporal regions for silent speechreading can be modulated by both hearing status and speechreading skill.

Keywords
Deafness
Brain
Language
Sign language
Speechreading
fMRI



1
Introduction
Scott & Johnsrude, 2003
Bernstein et al., 2002; Calvert et al., 1997; Calvert et al., 1999
Calvert, Campbell, & Brammer, 2000
Ludman et al., 2000; MacSweeney et al., 2000; Paulesu et al., 2003; Pekkola et al., 2005
Ruytjens, Albers, van Dijk, Wit, & Willemsen, 2006
Buccino et al., 2004; Campbell et al., 2001
Nishitani & Hari, 2002
Paulesu et al., 2003
Watkins, Strafella, & Paus, 2003
Hall, Fussell, and Summerfield (2005)
 did not find marked activation in the superior temporal gyrus at the group level when hearing adults observed silently spoken sentences, as compared to viewing facial gurning. However, their participants varied greatly in their ability to speechread, and a positive correlation was found between activation in the left posterior superior temporal gyrus and speechreading skill.
Bernstein, Demorest, & Tucker, 2000
Mohammed, Campbell, MacSweeney, Barry, & Coleman, 2006
MacSweeney et al., 2001; MacSweeney et al., 2002
n
MacSweeney et al.'s (2002)
Sadato et al. (2005)
, reported activation in superior temporal regions in both hearing and deaf participants viewing speech-like actions. Here, the stimulus was a cartoon avatar opening and closing its mouth to form different vowel-like patterns, which participants may have interpreted as phonological gestures.
Mohammed et al., 2006
). By ‘partialling out’ individual differences in speechreading ability, we hoped to establish whether activation in brain regions could be modulated as a function of hearing status, irrespective of speechreading skill. Second, we used correlational analysis to establish, for each group in turn, which regions were sensitive to variations in speechreading skill.
To summarise, this study examines cortical correlates for the perception of lists of speechread words under lexical target detection conditions. We aimed to identify regions that may be activated during observation of silently spoken lexical items that are not drawn from a closed set, and when the contrast (baseline) condition was a speaker at rest. The questions posed were: (1) To what extent do prelingually deaf people who are proficient signers and speechreaders show activation in superior temporal regions, including auditory cortical processing regions? (2) Are the patterns of activation different in deaf and hearing people? (3) In which regions is speechreading ability positively correlated with activation?

2
Method
2.1
Participants
Mohammed et al., 2006
t
p
t
p
Mohammed et al., 2006
Table 1
z
Mohammed et al.'s (2006)
Table 1
).
All participants gave written informed consent to participate in the study according to the Declaration of Helsinki (BMJ 1991; 302: 1194) and the study was approved by the Institute of Psychiatry/South London and Maudsley NHS Trust Research Ethics Committee.

2.2
Stimuli
Stimuli were full-colour motion video of silently mouthed English words. Stimuli were modelled by a deaf native signer of BSL, who spoke English fluently (i.e., a BSL-English bilingual). The model was viewed full-face and torso. The words to be speechread were piloted on adult hearing volunteers who were not scanned. The final stimuli comprised only those words that were speechreadable by the hearing pilots. Stimuli consisted of both content words (nouns) and descriptive terms (both adjectival and adverbial).

2.3
fMRI experimental design and task
The speechreading task was one of four conditions presented to participants. The other three conditions comprised signed language (BSL) material (not reported here). The speech stimuli were presented in blocks, alternating with blocks of the other three experimental conditions (30-s blocks for each condition), and with a 15-s baseline condition. The total run duration for all four conditions and baseline was 15 min. Both deaf and hearing participants were given the same target-detection task and instructions. During the speechreading condition, participants were instructed to watch the speech patterns produced by the model and to try to understand them. They were required to make a push-button response whenever the model was seen to be saying ‘yes’. This relatively passive task was chosen in preference to a ‘deeper’ processing task (such as semantic classification) for several reasons. First, it allowed for relatively automatic processing of non-target items to occur (as confirmed in post-scan tests). Second, it ensured similar difficulty of the task across stimulus conditions. As hearing non-signers would not be able to perform a semantic task on the sign stimuli, using a sparse target detection task enabled all participants to perform the same task during all experimental conditions. Over the course of the experiment, participants viewed 96 stimulus items, 24 in each of the four experimental conditions. Items were not repeated within the same block and were pseudorandomised to ensure that repeats were not clustered at the end of the experiment. Each participant saw five blocks of the speechreading condition.
The baseline condition comprised video of the model at rest. The model's face and torso were shown, as in the experimental conditions. During the baseline condition, participants were directed to press a button when a grey fixation cross, digitally superimposed on the face region of the resting model, turned red. To maintain vigilance, targets in both the experimental and baseline conditions occurred randomly at a rate of one per block. Prior to the scan, participants practiced the tasks and were shown examples of the ‘yes’ targets outside the scanner using video of a model and words that were similar but not identical to those used in the experiment. Following the experiment, a sample of the hearing participants (8 of 13) and all of the deaf participants were asked to identify the items they had seen.
Stimuli in the experimental conditions appeared at a rate of 15 items per block. The rate of articulation across all experimental conditions, including the speechreading blocks, was approximately one item every 2 s. All stimuli were projected onto a screen located at the base of the scanner table via a Sanyo XU40 LCD projector and then projected to a mirror angled above the participant's head.

2.4
Imaging parameters
T
2
*



Talairach & Tournoux, 1988
). These comprised 40 near-axial 3 mm slices (0.3 mm gap), which were acquired parallel to the AC-PC line. The field of view for these scans was matched to that of the fMRI scans, but the matrix size was increased to 128 × 128, resulting in an in-plane voxel size of 1.875 mm. Other scan parameters (TR = 3 s, TE = 40 ms, flip angle = 90°) were, where possible, matched to those of the main EPI run, resulting in similar image contrast.

2.5
Data analysis
F
Edgington (1995)
 that avoids the necessity of calculating the residual degrees of freedom of the time series following model fitting.
Bullmore et al. (2001)
Donoho and Johnstone (1994)
) were removed. These were replaced by the threshold value. This step reduces the likelihood of refitting large, experimentally unrelated components of the signal following permutation.
Bullmore et al., 2001
Talairach & Tournoux, 1988
Brammer et al., 1997; Bullmore et al., 1996
T
2
*



-weighted images using a rigid body transformation. Second, an affine transformation to the Talairach template was computed. The cost function for both transformations was the maximization of the correlation between the images. Voxel size in Talairach space was 3 mm × 3 mm × 3 mm.

2.6
Group analysis
Bullmore et al., 1999
).

2.7
ANOVA
Y
a
bX
e
Y
X
a
b
e
b
b
b
Bullmore et al., 1999
).

2.8
ANCOVA
Table 1
R
R
a
a
H
a
X
e
H
X
e
a
a
H
).

2.9
Correlational analysis
z
r
 was significant, were computed such that the expected false positive rate was <1 cluster per brain.


3
Results
3.1
Behavioural data
t
p
t
p
t
p
 = 0.001). The behavioural data suggest that deaf participants’ greater accuracy in identification of non-target items (as indicated by the post-scan test) may have interfered with their processing of the target (as indicated by the relatively slow reaction times to targets in the scanner).

3.2
fMRI data
3.2.1
Speechreading vs. baseline
Table 2
Fig. 1
). In deaf participants, activation in the left superior temporal cortex was focused at the border between the posterior superior temporal gyrus and the transverse temporal gyrus (BA 42/41) and extended to the middle (BA 21) and inferior (BAs 37, 19) temporal gyri and the supramarginal gyrus (BA 40). This cluster of activation also extended to inferior (BAs 44, 45) and middle (BAs 6, 9) frontal gyri and precentral gyrus (BA 4). In the right hemisphere, a cluster of activation focused in the superior/middle temporal gyri (BA 22/21) extended to BAs 42 and 41 and posterior inferior temporal gyrus (BAs 37, 19). Activation in the right frontal cortex was focused in the precentral gyrus (BA 6) and extended to the inferior (BAs 44, 45) and middle (BAs 46, 9) frontal gyri. Additional activation was observed at the border of the medial frontal gyrus and the anterior cingulate gyrus (BA 6/32).

3.2.2
Deaf vs. hearing
x
y
z
x
y
z
x
y
z
 = 26).
z
x
y
z
Penhune, Zatorre, MacDonald, and Evans (1996)
Westbury, Zatorre, and Evans (1999)
Fig. 2
). No brain regions were significantly more active in hearing than deaf participants when speechreading was a covariate in the analysis.

3.2.3
Cortical activation for speechreading: correlations with speechreading skill
Table 1
z
-scores in both deaf and hearing groups.

3.2.4
Deaf group
Table 3
Talairach and Tournoux (1988)
Penhune et al. (1996)
Penhune et al., 1996
). Both clusters extended to include the posterior superior temporal gyrus (BAs 42, 22). Additional areas showing significant correlation included the middle portion of the right middle temporal gyrus (BA 21). In the frontal cortex, correlations were observed in the middle frontal gyri of both hemispheres (BA 6). In the right hemisphere, correlations were also observed in the dorsolateral prefrontal cortex (BA 46), precentral gyrus (BA 6/4) and in the anterior insula. Additional correlations were observed in the anterior cingulate gyrus (BA 32/24) and the cerebellum.

3.2.5
Hearing group
z
-scores included the fusiform (BA 37) and lingual (BA 18) gyri of the right hemisphere and the right postcentral gyrus (BA 4). Additional positive correlations were observed in the posterior cingulate gyrus (BA 23).



4
Discussion
Table 1
Bernstein et al., 2000; Mohammed et al., 2006
). Deaf people, including deaf people who use a signed language, rely on speechreading, whether hearing-aid supported or un-aided, to communicate in the wider hearing community. In contrast, in hearing people, where the auditory channel dominates for speech identification, reliance on silent seen speech is generally unfamiliar and unpractised. In the present study most participants, whether deaf or hearing, could speechread much of the spoken material, and it can be assumed, therefore, that some of what they were shown in the scanner was lexically processed—albeit more in deaf than in hearing participants. Interpretation of the imaging data must bear these considerations in mind. Covariance and correlational analyses allow the behavioural and neuroimaging results to be aligned.
Zeki et al., 1991
Calvert et al., 1997; Paulesu et al., 2003
Pekkola et al., 2005
Buccino et al., 2004; Campbell et al., 2001; Paulesu et al., 2003; Watkins et al., 2003
) and may reflect the operation of mirror neuron systems in the observation of speech actions.
Sadato et al. (2005)
MacSweeney et al., 2001; MacSweeney et al., 2002
MacSweeney et al., 2002
). A further study involving a larger group of deaf participants, and manipulating task, baseline condition and stimuli, will help establish whether our previous studies simply lacked power or whether task and stimulus factors systematically affect the extent to which superior temporal regions are recruited during silent speechreading in those born profoundly deaf.
4.1
Deaf vs. hearing
Fig. 2
Calvert et al., 1999; Calvert et al., 2000
greater
 in deaf than hearing people. One possibility is that activation by seen speech in p-STS is sensitive to the dominant speech modality within this multimodal region. That is, activation by silent speech in this region may be greater in deaf people because the region has developed to be sensitive to visual speech, while for hearing people it has developed to be sensitive to auditory speech characteristics, with visual speech as a secondary function. Structural imaging of the connections between p-STS and visual and auditory cortices in deaf and hearing individuals could be employed to test this hypothesis.
Fine, Finney, Boynton, & Dobkins, 2005
Finney, Fine, & Dobkins, 2001
Sadato et al., 2005
Bavelier, Dye, & Hauser, 2006
MacSweeney et al., 2004
) suggest that perception of signed language, and even of non-linguistic biological movement, can recruit regions within superior temporal cortex to a greater extent in deaf native signers than in hearing people exposed to a signed language from birth (hearing native signers).

4.2
Correlations of activation with individual differences in speechreading skill
r
p
r
p
z
) derived for each group formed the basis for exploring the relationship between speechreading skill and cortical activation. Within each group, different patterns of association were observed. In deaf participants, the correlational analyses showed that activation in the posterior portion of the superior temporal gyri (as well as middle temporal and middle frontal gyri) was positively associated with speechreading.
Hall et al. (2005)
. Additional activations displaying a positive correlation with speechreading skill included the right postcentral and inferior temporal (fusiform) gyri, perhaps suggesting relatively greater involvement of articulatory skill and face processing in hearing individuals’ speechreading, respectively.
n
Hall et al. (2005)
 did not find reliable activation in superior temporal gyrus for silent speechreading in contrast to viewing facial gurning in a group of 33 hearing participants, who also varied widely in speechreading skill. However, they did report a reliable positive correlation between speechreading skill and activation in this region. The inference from that study together with the present one must be that, when speechread material is linguistically processed, superior temporal regions within the left hemisphere are likely to be recruited. Additionally, the present study shows that it was deaf rather than hearing people who showed this relationship most clearly, and where individual differences in speechreading skill made an additional impact, despite the range of speechreading skill being larger in the hearing than the deaf group.
Chan, Chan, Kwok, & Yu, 2000
Rhoades & Chisholm, 2000
). Thus, a neurological hypothesis is being advanced which suggests that the deaf child should not watch spoken (or signed) language since this may adversely affect the sensitivity of auditory brain regions to acoustic activation following cochlear implantation. Such advice may not be warranted if speechreading activates auditory regions in both deaf and hearing individuals.
Bergeson, Pisoni, & Davis, 2005
). The possibility that superior temporal regions in deaf individuals, once tuned to visible speech, may then more readily adapt to perceiving speech multimodally should be seriously considered when recommendations concerning pediatric cochlear implantation procedures are being developed.



Acknowledgements
This research was supported by the Wellcome Trust (Project Grant 068607/Z/02/Z ‘Imaging the Deaf Brain’). The Wellcome Trust also supports M.M. (Career Development Fellowship). R.C. and B.W. are further supported by Grant RES 620-28-6001 from the ESRC (Deafness, Cognition and Language (DCAL) Research Centre). We are grateful to the following people for their assistance: Jordan Fenlon, Tyron Woolfe, Marc Seal, Cathie Green, Maartje Kouwenberg, Zoë Hunter and Karine Gazarian.

1
y
t
y
t
y
t
ay
t
y
t
ay
t
a
 is a constant (0.3) derived from analysis of observed curve shapes.

2
Penhune et al. (1996)
. Thirty-one voxels in the deaf group and four voxels in hearing group displayed ≥50% probability of being located within this region.


References
Bavelier et al., 2006
Bavelier
D.

Dye
M.W.

Hauser
P.C.


Do deaf individuals see better?
Trends in Cognitive Science
2006
10
512
518


Bergeson et al., 2005
Bergeson
T.R.

Pisoni
D.B.

Davis
R.A.


Development of audiovisual comprehension skills in prelingually deaf children with cochlear implants
Ear and Hearing
2005
26
149
164
15809542


Bernstein et al., 2002
Bernstein
L.E.

Auer
E.T.

Moore
J.K.

Ponton
C.W.

Don
M.

Singh
M.


Visual speech perception without primary auditory cortex activation
NeuroReport
2002
13
311
315
11930129


Bernstein et al., 2000
Bernstein
L.E.

Demorest
M.E.

Tucker
P.E.


Speech perception without hearing
Perception & Psychophysics
2000
62
233
252
10723205


Brammer et al., 1997
Brammer
M.J.

Bullmore
E.T.

Simmons
A.

Williams
S.C.

Grasby
P.M.

Howard
R.J.


Generic brain activation mapping in functional magnetic resonance imaging: A nonparametric approach
Magnetic Resonance Imaging
1997
15
763
770
9309607


Buccino et al., 2004
Buccino
G.

Lui
F.

Canessa
N.

Patteri
I.

Lagravinese
G.

Benuzzi
F.


Neural circuits involved in the recognition of actions performed by nonconspecifics: An fMRI study
Journal of Cognitive Neuroscience
2004
16
114
126
15006041


Bullmore et al., 1996
Bullmore
E.T.

Brammer
M.

Williams
S.C.

Rabe-Hesketh
S.

Janot
N.

David
A.


Statistical methods of estimation and inference for functional MR image analysis
Magnetic Resonance in Medicine
1996
35
261
277
8622592


Bullmore et al., 2001
Bullmore
E.T.

Long
C.

Suckling
J.

Fadili
J.

Calvert
G.

Zelaya
F.


Colored noise and computational inference in neurophysiological (fMRI) time series analysis: Resampling methods in time and wavelet domains
Human Brain Mapping
2001
12
61
78
11169871


Bullmore et al., 1999
Bullmore
E.T.

Suckling
J.

Overmeyer
S.

Rabe-Hesketh
S.

Taylor
E.

Brammer
M.J.


Global, voxel, and cluster tests, by theory and permutation, for a difference between two groups of structural MR images of the brain
IEEE Transactions on Medical Imaging
1999
18
32
42
10193695


Calvert et al., 1999
Calvert
G.A.

Brammer
M.J.

Bullmore
E.T.

Campbell
R.

Iversen
S.D.

David
A.S.


Response amplification in sensory-specific cortices during crossmodal binding
NeuroReport
1999
10
2619
2623
10574380


Calvert et al., 1997
Calvert
G.A.

Bullmore
E.T.

Brammer
M.J.

Campbell
R.

Williams
S.C.R.

McGuire
P.K.


Activation of auditory cortex during silent lipreading
Science
1997
276
593
596
9110978


Calvert et al., 2000
Calvert
G.A.

Campbell
R.

Brammer
M.J.


Evidence from functional magnetic resonance imaging of crossmodal binding in the human heteromodal cortex
Current Biology
2000
10
649
657
10837246


Campbell et al., 2001
Campbell
R.

MacSweeney
M.

Surguladze
S.

Calvert
G.

McGuire
P.

Suckling
J.


Cortical substrates for the perception of face actions: An fMRI study of the specificity of activation for seen speech and for meaningless lower-face acts (gurning)
Brain Research. Cognitive Brain Research
2001
12
233
243
11587893


Chan et al., 2000
Chan
S.C.

Chan
S.K.

Kwok
I.C.

Yu
H.C.


The speech and language rehabilitation program for pediatric cochlear implantees in Hong Kong
Advances in Oto-Rhino-Laryngology
2000
57
247
249
11892159


Donoho and Johnstone, 1994
Donoho
D.L.

Johnstone
J.M.


Ideal spatial adaptation by wavelet shrinkage
Biometrika
1994
81
425
455


Edgington, 1995
Edgington
E.S.


third ed.
1995
Marcel Dekker, Inc.
New York/Basel


Fine et al., 2005
Fine
I.

Finney
E.M.

Boynton
G.M.

Dobkins
K.R.


Comparing the effects of auditory deprivation and sign language within the auditory and visual cortex
Journal of Cognitive Neuroscience
2005
17
1621
1637
16269101


Finney et al., 2001
Finney
E.M.

Fine
I.

Dobkins
K.R.


Visual stimuli activate auditory cortex in the deaf
Nature Neuroscience
2001
4
1171
1173


Friman et al., 2003
Friman
O.

Borga
M.

Lundberg
P.

Knutsson
H.


Adaptive analysis of fMRI data
Neuroimage
2003
19
837
845
12880812


Friston et al., 1998
Friston
K.J.

Josephs
O.

Rees
G.

Turner
R.


Nonlinear event-related responses in fMRI
Magnetic Resonance in Medicine
1998
39
41
52
9438436


Hall et al., 2005
Hall
D.A.

Fussell
C.

Summerfield
A.Q.


Reading fluent speech from talking faces: Typical brain networks and individual differences
Journal of Cognitive Neuroscience
2005
17
939
953
15969911


Ludman et al., 2000
Ludman
C.N.

Summerfield
A.Q.

Hall
D.

Elliott
M.

Foster
J.

Hykin
J.L.


Lip-reading ability and patterns of cortical activation studied using fMRI
British Journal of Audiology
2000
34
225
230
10997451


MacSweeney et al., 2000
MacSweeney
M.

Amaro
E.

Calvert
G.A.

Campbell
R.

David
A.S.

McGuire
P.


Silent speechreading in the absence of scanner noise: An event-related fMRI study
NeuroReport
2000
11
1729
1733
10852233


MacSweeney et al., 2002
MacSweeney
M.

Calvert
G.A.

Campbell
R.

McGuire
P.K.

David
A.S.

Williams
S.C.


Speechreading circuits in people born deaf
Neuropsychologia
2002
40
801
807
11900730


MacSweeney et al., 2001
MacSweeney
M.

Campbell
R.

Calvert
G.A.

McGuire
P.K.

David
A.S.

Suckling
J.


Dispersed activation in the left temporal cortex for speechreading in congenitally deaf speechreaders
Proceedings of the Royal Society of London B
2001
268
451
457


MacSweeney et al., 2004
MacSweeney
M.

Campbell
R.

Woll
B.

Giampietro
V.

David
A.S.

McGuire
P.K.


Dissociating linguistic and nonlinguistic gestural communication in the brain
Neuroimage
2004
22
1605
1618
15275917


Mohammed et al., 2006
Mohammed
T.

Campbell
R.

MacSweeney
M.

Barry
F.

Coleman
M.


Speechreading and its association with reading among deaf, hearing and dyslexic individuals
Clinical Linguistics & Phonetics
2006
20
621
630
17056494


Nishitani and Hari, 2002
Nishitani
N.

Hari
R.


Viewing lip forms: Cortical dynamics
Neuron
2002
36
1211
1220
12495633


Paulesu et al., 2003
Paulesu
E.

Perani
D.

Blasi
V.

Silani
G.

Borghese
N.A.

De Giovanni
U.


A functional-anatomical model for lipreading
Journal of Neurophysiology
2003
90
2005
2013
12750414


Pekkola et al., 2005
Pekkola
J.

Ojanen
V.

Autti
T.

Jaaskelainen
I.P.

Mottonen
R.

Tarkiainen
A.


Primary auditory cortex activation by visual speech: an fMRI study at 3 T
NeuroReport
2005
16
125
128
15671860


Penhune et al., 1996
Penhune
V.B.

Zatorre
R.J.

MacDonald
J.D.

Evans
A.C.


Interhemispheric anatomical differences in human primary auditory cortex: Probabilistic mapping and volume measurement from magnetic resonance scans
Cerebral Cortex
1996
6
661
672
8921202


Rhoades and Chisholm, 2000
Rhoades
E.A.

Chisholm
T.H.


Global language progress with an auditory-verbal approach for children who are deaf or hard of hearing
Volta Review
2000
102
5
24


Ruytjens et al., 2006
Ruytjens
L.

Albers
F.

van Dijk
P.

Wit
H.

Willemsen
A.


Neural responses to silent lipreading in normal hearing male and female subjects
The European Journal of Neuroscience
2006
24
1835
1844
17004947


Sadato et al., 2005
Sadato
N.

Okada
T.

Honda
M.

Matsuki
K.

Yoshida
M.

Kashikura
K.


Cross-modal integration and plastic changes revealed by lip movement, random-dot motion and sign languages in the hearing and deaf
Cerebral Cortex
2005
15
1113
1122
15563723


Scott and Johnsrude, 2003
Scott
S.K.

Johnsrude
I.S.


The neuroanatomical and functional organization of speech perception
Trends in Neuroscience
2003
26
100
107


Talairach and Tournoux, 1988
Co-planar stereotaxic atlas of the human brain
 (M. Rayport, Trans.). New York: Thieme Medical Publishers, Inc.

Watkins et al., 2003
Watkins
K.E.

Strafella
A.P.

Paus
T.


Seeing and hearing speech excites the motor system involved in speech production
Neuropsychologia
2003
41
989
994
12667534


Westbury et al., 1999
Westbury
C.F.

Zatorre
R.J.

Evans
A.C.


Quantifying variability in the planum temporale: A probability map
Cerebral Cortex
1999
9
392
405
10426418


Zeki et al., 1991
Zeki
S.

Watson
J.D.

Lueck
C.J.

Friston
K.J.

Kennard
C.

Frackowiak
R.S.


A direct demonstration of functional specialization in human visual cortex
Journal of Neuroscience
1991
11
641
649
2002358




Fig. 1
p
p
-value = 0.0025. Activations on lateral renderings are displayed up to 15 mm beneath the cortical surface. Five sequential axial sections, showing activation in superior temporal regions, including the planum temporale (PT) and Heschl's gyrus (HG) are also displayed.



Fig. 2
z
p
p
-value = 0.01). No regions were more active for hearing than deaf participants. No right hemisphere regions were significantly different across the groups. Five sequential axial sections, showing activation in superior temporal regions, including the planum temporale (PT) and Heschl's gyrus (HG) are also displayed.



Table 1
z
-scores


Age
NVIQ centile
TAS
z
-score


n
 = 13, 6 female)
27.4 (7.76) range: 18–49
88.2 (13.3) range: 50–98
32.54 (3.07) range: 27–37
0.436 (0.59) range: −0.62–1.29

n
 = 13, 6 female)
29.4 (6.15) range: 18–43
83.2 (19.6) range: 25–99
25.08 (4.89) range: 15–34
0.183 (1.07) range: −2.02–2.14




Table 2
Activated regions for the perception of speech compared to baseline (static model) in deaf and hearing participants


Hemisphere
Size (voxels)
x
y
z

BA


Deaf group

 Superior/middle temporal gyrus
R
246
51, −7, −3
22/21

 Superior/transverse temporal gyrus
L
916
−54, −22, 10
42/41

 Precentral gyrus
R
237
47, −4, 40
6

 Medial frontal gyrus/anterior cingulate gyrus
L
211
−4, 15, 43
6/32

  

Hearing group

 Superior/middle temporal gyrus
R
451
43, −30, 0
22/21

 Middle temporo-occipital junction
L
493
−43, −63, 0
37

 Supramarginal gyrus
L
125
−33, −52, 43
40

 Supramarginal gyrus
R
220
36, −48, 40
40

 Precentral gyrus
L
457
−47, −7, 43
4/6

 Inferior frontal gyrus
R
521
40, 11, 26
44

 Medial frontal gyrus
R
218
4, 4, 50
6



p
p
-value = 0.0025. Foci correspond to the most activated voxel in each 3-D cluster.



Table 3
z
-scores) in deaf and hearing participants


Hemisphere
Size (voxels)
x
y
z

BA


Deaf group

 Cerebellum
L
8
−11, −44, −30
–

 Middle temporal gyrus
R
16
47, −26, −3
21

 Insula
R
6
36, 15, 7
–

 Transverse temporal gyrus
R
7
47, −19, 13
41

 Superior temporal gyrus
L
16
−54, −26, 13
42

 Dorsolateral prefrontal cortex
R
10
47, 22, 26
46

 Anterior cingulate gyrus
–
7
0, 15, 33
32/24

 Precentral gyrus
R
5
29, −7, 50
6/4

 Middle frontal gyrus
L
6
−29, −4, 53
6

 Middle frontal gyrus
R
6
33, 0, 53
6

  

Hearing group

 Fusiform gyrus
R
9
33, −44, −13
37

 Lingual gyrus
R
8
11, −81, −7
18

 Posterior cingulate gyrus
–
6
0, −33, 23
23

 Posterior cingulate gyrus
L
5
−4, −11, 30
23

 Postcentral gyrus
R
11
51, −15, 30
4



p
p
-value = 0.0025. Foci correspond to the most activated voxel in each 3-D cluster.





