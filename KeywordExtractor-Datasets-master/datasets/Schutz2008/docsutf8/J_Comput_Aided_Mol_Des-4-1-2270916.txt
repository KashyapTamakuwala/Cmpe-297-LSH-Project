J Comput Aided Mol Des
Journal of Computer-Aided Molecular Design
0920-654X
1573-4951
Springer Netherlands
Dordrecht


2270916
18217218
9166
10.1007/s10822-007-9166-3
Article


How to do an evaluation: pitfalls and traps

Hawkins
Paul C. D.

+1-505-4737385
+1-505-4730833
phawkins@eyesopen.com



Warren
Gregory L.



Skillman
A. Geoffrey



Nicholls
Anthony



OpenEye Scientific Software, 9, Bisbee Court, Suite D, Santa Fe, NM 87508 USA 

23
1
2008

3
2008

22
3-4
179
190
28
11
2007

18
12
2007


© The Author(s) 2008

The recent literature is replete with papers evaluating computational tools (often those operating on 3D structures) for their performance in a certain set of tasks. Most commonly these papers compare a number of docking tools for their performance in cognate re-docking (pose prediction) and/or virtual screening. Related papers have been published on ligand-based tools: pose prediction by conformer generators and virtual screening using a variety of ligand-based approaches. The reliability of these comparisons is critically affected by a number of factors usually ignored by the authors, including bias in the datasets used in virtual screening, the metrics used to assess performance in virtual screening and pose prediction and errors in crystal structures used.

Keywords
Software evaluation
Pose prediction
Coordinate error
Virtual screening
Property bias

issue-copyright-statement
© Springer Science+Business Media B.V. 2008




Introduction
1
2
3
4
6
7
8
]. In the following sections some issues with studies on pose prediction and virtual screening will be discussed.

Pose prediction
9
10
], comparing docking programs for their ability to predict the bioactive pose of a ligand is difficult for a number of reasons, some of which are obvious while others are subtler. However, the operational difficulties in comparing programs in a robust way should not cause other sources of error to be ignored. It is a truism that it is meaningless to compute a property with greater precision than the accuracy of the experiment that measures that property. Unfortunately, as will be seen in subsequent sections, this is often ignored by authors of papers in the area of pose prediction.
11
8
1
10
Table 1
Comparison of RMSD results from a set of docking engines


MolDock
GLIDE
Surflex


Mean
1.38
1.38
1.86

SD
1.49
1.74
2.02

Median
0.92
0.69
1.10





11
]. As such this is not an “apples to apples” comparison, since the Glide pre-processing step optimizes protein and ligand coordinates using the force-field component of the Glide scoring function, which necessarily introduces bias in the structure. In the MolDock case the authors have essentially trained the MolDock fitness function on the 77 complexes that they use to evaluate its performance. As such the reported results give no indication of the likelihood of success in predicting a pose for a system upon which MolDock was not trained.
12
10
model
13
14
], try to reflect the ultimate use of the predicted pose, i.e., determining the nature of the interactions that the ligand makes with the protein. While the IBAC approach has value in that it assesses a computed pose by its interactions with the protein, it is not amenable to automation; it is therefore tedious to assemble sufficient data to make statistically robust comparisons between tools based on IBAC.
However, the problem of choosing which metric to use to compare pose prediction studies is dwarfed by the difficulty in choosing a dataset of protein–ligand co-complexes upon which to perform the comparison. A widespread tendency in conformer reproduction and pose prediction studies is to ignore even the possibility of error in the crystal structures that are being reproduced. Crystal structures are often treated as perfect, infinitely precise and accurate representations of the atomic details of a protein–ligand complex. There are a number of reasons why this is not so; a few will be discussed in the following paragraphs.

Crystal structures are models
1
Fig. 1
Fitting atoms into electron density produces a crystallographic model




Incomplete or fragmentary density;

The electron density not defining the positions of all atoms unambiguously;

Poor structural parameters are used for the fitting process, which can give inappropriate conformations (particularly of ligands);

Errors by the users, arising from careless treatment of the data or lack of expertise with small molecules.



15
16
R
free
17
R
free
R
free
18
R
free
R
free
 as one of a set of metrics for selection of crystal structures could help to avoid the selection of poorly fitted models for pose reproduction.

Crystal structures have unavoidable imprecision
R
free
19
20
2
21
22
Fig. 2
B-factors for the ligand in the 5ER1 crystal structure




23
24
18
1
1
.
1
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ \sigma {\left( {r{\text{, }}B_{{{\text{avg}}}} } \right)} = 2.2 N{_{\text{atoms}}} ^{{1/2}} V{_{{\text{a}}}} ^{{1/2}} n{_{{{\text{obs}}}}} ^{ - 5/6} R_{{{\text{free}}}} $$\end{document}


r
N
atoms
V
a
n
obs
R
free
1
r
r
), when comparing a computed and an experimental structure.
r
4
3
3
Fig. 3
4
]




3
2
Table 2
Resolution and DPI for selected structures from the Kirchmair dataset

PDB code
Resolution (Å)
DPI (Å)


1FC7
1.38
0.69

1FDO
1.38
0.60

1JJT
1.8
1.37

1JJE
1.8
1.25

1CIB
2.5
5.54

1ILH
2.76
0.14

1C8M
2.8
0.18

1QJX
2.8
0.25





18
] the assumption is made that the errors in the computed pose are the same as for the experimental pose. In this analysis a computed and experimental pose must be different by an RMSD of √2 × DPI for the difference to be significant, which would mean that an even higher proportion of the poses in the Kirchmair set have been reproduced with a greater precision than the experimental accuracy.
4
25
3
2
2
24
Fig. 4
The nominal resolution versus the coordinate error for a subset of the Gold (structures with resolution <2.5 Å) and the Glide data sets




2
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ \sigma {\left( {r{\text{, }}B_{{{\text{avg}}}}}\right)} = 0.22{\left({1 + s}\right)}^{{1/2}}V{_{M}}^{{-1/2}} C^{{-5/6}}R_{{{\text{free}}}} d{_{{\min }}}^{{5/2}} $$\end{document}


2
s
V
m

C
d
min
4
2
s
V
m

C
R
free
 is equal to the resolution/10.
4
 shows that the GOLD set contains one structure (1YEE) whose calculated DPI lies below the theoretical lower limit. This is probably due to a mistranscription error in the PDB file. The PDB file for the 1YEE structure gives the number of reflections as 77209, while the number of unique reflections is 21342—a disparity that cannot be reconciled by reference to the data redundancy for this structure. Accordingly the calculated DPI for 1YEE is too low because the reported number of reflections is erroneously high. In the GLIDE set all structures have DPIs higher than our predicted theoretical minimum. In fact, in this set 52% of the structures have a DPI > 0.5 Å, and in 31% of the cases the reported RMSD for redocking is less than the DPI (so that the prediction is more precise than experimental accuracy allows). This is only one example of a publication in which protein structures are assumed to be free of uncertainty in their atomic positions; the literature abounds with others.
R
free
overall
26
]. Accordingly, the results from these studies should be treated with some caution.

Crystal structures have avoidable errors
R
free
27
5
6
Fig. 5
Ligand conformation from 1A8T structure. The conformation has two serious atom–atom clashes



Fig. 6
Ligand conformation from the 1A4K structure. The cis-amide group is an error of fitting




5
28
]). The deposited coordinates are clearly in error in this case and no docking engine or conformer generator should be expected to reproduce such a structure.
6
 for the 1A4k structure is of a more subtle nature. Here the crystallographer has fitted a highly strained cis-amide into the electron density with no compelling reason from the electron density to do so. The amide group is packing against a tyrosine residue from a symmetry mate in the unit cell, and makes no polar interaction with it. The corresponding trans-amide, which fits the experimental density just as well, is 15.5 kcal/mol lower in energy (using the MMFF94 force-field). Also this trans amide is able to make a hydrogen bond (with a backbone carbonyl group) that is not available to the cis conformation. Once again, this model should not be reproduced by a docking engine or conformer generator, as it is obviously not the correct solution.
29
30
31
32
]. Those ligand conformations with strain energies higher than 10 kcal/mol are almost certainly incorrect. As such low ligand strain should be among the criteria for selection of structures for pose prediction, as structures with high strain are very likely to arise from errors in the fitting process.
33
].

Virtual screening
34
36
], the vast majority have been concentrated in the area of retrospective virtual screening. In the rest of this article we shall concern ourselves solely with the retrospective experiments. The goal of such experiments is often to identify an application that performs well on a given target, or across a wide range of targets, with a view to utilizing this application in prospective virtual screens.
3
3
sampled
x%
sampled
x%
total
total
 = number of compounds in entire database).
3
\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$ {\text{EF}} = (\text{Hits}{_{\text{sampled}}}^{\text{x\%}}/{\text{N}}{_{\text{sampled}}}^{\text{x\%}}) \times {\text{(N}}_{{{\text{total}}}} {\text{/Hits}}_{{{\text{total}}}} {\text{) }} $$\end{document}


It is dependent on the structure of the dataset, in that datasets with larger proportions of actives will have a narrower range of possible enrichments.

It penalizes ranking one active compound above another.

It exhibits pernicious behaviour at the cut-off at which the enrichment is calculated.

It gives no weight to where in the ranked list a known active compound appears. Thus to calculate enrichment at 1% in a virtual screen of 10,000 compounds, the number of actives (N) in the top ranked 100 compounds is needed. However the enrichment at 1% is the same whether the N active compounds are ranked at the very top of the list or at the very bottom of the top ranked 100.

It is difficult to calculate analytically errors in enrichment, and there is no available literature for such a calculation.



37
37
38
39
40
3
]. These metrics have, historically, only been used by the groups that invented them and are thus not useful for comparison to studies conducted by other groups. This lack of direct comparability across studies is compounded by the fact that these metrics cannot be converted into a more commonly used metric that could be used to compare results. Also, as with enrichment, it is unknown how to estimate analytically errors in any of these metrics.
41
42
2
43
that have not yet been conducted
. This predictive ability is not provided by metrics such as enrichment, cumulative probability and average number of outranking decoys, because while the ROC describes a property of the application studied, the other metrics essentially describe a property of the experiment. The AUC assesses virtual screening performance across the entire database and many practitioners of virtual screening are, rightly, most concerned about early performance of the tools they use. This is one reason why enrichment is so commonly used to measure success. The metric of early performance based on the ROC curve is the true positive rate at fixed false positive rates. The true positive rate at a false positive rate of, for example, 1% is a much more robust measure than the enrichment at 1% and provides similar information about the early performance of a tool.
41
]. This is not a property possessed by enrichment, average number of outranking decoys etc. For these metrics errors can only be estimated (by bootstrapping or other approaches) from the raw data, which is rarely provided. The error in the AUC, as with other metrics, is reduced by increasing the number of positives (active compounds) and by increasing the number of negatives (inactives). The Hanley treatment shows that the error in the AUC is most significantly reduced by increasing the number of actives, while increasing the number of decoys has a much smaller impact on the error. Therefore virtual screening datasets with high proportions of active compounds will provide results with lower error bars for the AUC. With the error for an AUC available, meaningful comparisons can be made between two or more different tools. However the other metrics used for virtual screening mentioned above do not allow an analytical estimation of their errors. Comparisons designed to determine which tool is superior for a given purpose, that are based on metrics assumed to be free of errors, are fraught with difficulty.
44
2
vide infra
vide supra
45
43
37
].
46
7
47
48
Fig. 7
Effect of decoy selection method on virtual screening by docking




8
2
49
46
50
51
Fig. 8
AUCs for various virtual screening methods on part of the Surflex-Dock validation set




8
 that in four of the eight cases (OPPA, HIV-PR, TK and PARP) the active compounds are very dissimilar from the background set, as the 1D ranking method gives very good virtual screening performance. In a fifth case (TS) the performance of 1D method is as good as any of the 3D methods, although none of the tools perform particularly well. Accordingly, judging virtual screening performance for any tool using such datasets is unlikely to be productive as most of the “signal” separating actives from decoys lies purely in differences in simple molecular properties. A further confounding issue with these datasets is that a large number of the active compounds in these sets are close structural analogues of one another. For ligand-based methods this high structural similarity amongst the actives can cause the actives to be very easily discriminated from the decoys, while a structure-based method may have more difficulty. Accordingly, while property bias is an important consideration in constructing decoy sets, analogue bias should be carefully considered when selecting sets of active compounds.
52
decoy
active
 selection is striking. It is a topic of further investigation whether much of the reported success of the DACCS method is due to over-training/poor active compound selection.
4
53
8
54
]. This makes “DUD 2.0” a suitable dataset not only for docking approaches but also ligand-based techniques. It should be noted that there is a limit to the acceptable level of similarity between actives and presumptive decoys. When the decoys are too similar to the actives the assumption that the decoys are inactive becomes increasingly untenable, giving rise to large numbers of “false false positives”. Accordingly the problem of decoy selection is not yet completely solved, and may not admit of a single solution for all problems or tools. However, since in retrospective work the point is purely to gain a measure of the expectation of performance in as yet unperformed studies, the use of carefully designed decoy sets is mandatory.
It is unfortunate that the docking targets in DUD (39 crystal structures and 1 homology model) were not selected with as much care as the small molecule datasets. In 6 of the 38 co-crystal structures in DUD (there is one apo structure in the set), the DPIs are 1.5 Å or more, resulting in significant uncertainty in the positioning of any atom in these structures. These structures are ALR2 (1AH3), COX-2 (1CX2), EGFR (1M17), GR (1M2XZ), InhA (1P44) and p38 (1KV2). Accordingly docking results from these structures should be interpreted with great care.

Conclusions
R
free
52
55
]. It is hoped that the field will soon converge to a single metric of virtual screening performance, such as the ROC, that will allow robust and direct comparisons between tools and between studies.


Acknowledgement
The authors wish to thank Annie Lux, MFA for editing and proof-reading of the manuscript.
Open Access
 This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.

References
1.
Warren
G

Webster Andrews
C

Capelli
A-M

Clarke
B

LaLonde
J

Lambert
MH

Lindvall
M

Nevins
N

Semus
S

Senger
S

Tedesco
G

Wall
ID

Woolven
JM

Peishoff
CE

Head
MSJ


Med Chem
2006
49
5912
10.1021/jm050362n

Warren G, Webster Andrews C, Capelli A-M, Clarke B, LaLonde J, Lambert MH, Lindvall M, Nevins N, Semus S, Senger S, Tedesco G, Wall ID, Woolven JM, Peishoff CE, Head MSJ (2006) Med Chem 49:5912 

2.
Pham
TA

Jain
AN


J Med Chem
2006
49
5856
10.1021/jm050040j

17004701


3.
Freisner
RA

Murphy
RB

Repasky
MP

Frye
LL

Greenwood
JR

Halgren
TA

Sanschagrin
PC

Mainz
DT


J Med Chem
2007
47
1564

Freisner RA, Murphy RB, Repasky MP, Frye LL, Greenwood JR, Halgren TA, Sanschagrin PC, Mainz DT (2007) J Med Chem 47:1564 

4.
Kirchmair
J

Wolber
G

Laggner
C

Langer
TJ


Chem Inf Model
2006
46
1848
10.1021/ci060084g

Kirchmair J, Wolber G, Laggner C, Langer TJ (2006) Chem Inf Model 46:1848 

5.
Bostrom
J

Greenwood
JR

Gottfries
J


J Mol Graphics Model
2003
21
449
10.1016/S1093-3263(02)00204-8

Bostrom J, Greenwood JR, Gottfries J (2003) J Mol Graphics Model 21:449 

6.
Bohme-Leite
T

Gomes
D

Miteva
MA

Chomilier
J

Villoutreix
BO

Tuffery
P


Nucleic Acids Res
2007
35
W568
10.1093/nar/gkm289

17485475


7.
Hawkins
PCD

Skillman
AG

Nicholls
A


J Med Chem
2007
50
74
10.1021/jm0603365

17201411


8.
Evans
DA

Doman
TN

Thorner
DA

Bodkin
MJ


J Chem Inf Model
2007
47
1248
10.1021/ci7000082

17477520


9.
Erickson
JA

Jalaie
M

Robertson
DH

Lewis
RA

Vieth
M


J Med Chem
2004
47
45
10.1021/jm030209y

14695819


10.
Cole
JC

Murray
CW

Nissink
JWM

Taylor
RD

Taylor
R


Proteins: Struct Funct Bioinformat
2005
60
325
10.1002/prot.20497

Cole JC, Murray CW, Nissink JWM, Taylor RD, Taylor R (2005) Proteins: Struct Funct Bioinformat 60:325 

11.
Thomsen
R

Christensen
MK


J Med Chem
2006
49
3315
10.1021/jm051197e

16722650


12.
Abagyan
RA

Totrov
MM


J Mol Biol
1997
268
678
10.1006/jmbi.1997.0994

9171291


13.
http://www.eyesopen.com/about/events/cup7/schmitt/060308_present4CUP7_SS.pdf


14.
Kroemer
RT

Vulpetti
A

McDonald
JJ

Rohrer
DC

Trosset
J-Y

Giordanetto
F

Cotesta
S

McMartin
C

Kihlen
M

Stouten
PFW


J Chem Inf Comput Sci
2004
44
871
10.1021/ci049970m

15154752


15.
Kleywegt
GJ


Acta Cryst
2000
D56
249

Kleywegt GJ (2000) Acta Cryst D56:249 

16.
Nissink
JMW

Murray
C

Hartshorn
M

Verdonk
ML

Cole
JC

Taylor
R


Proteins: Struct Funct Genetics
2002
49
457
10.1002/prot.10232

Nissink JMW, Murray C, Hartshorn M, Verdonk ML, Cole JC, Taylor R (2002) Proteins: Struct Funct Genetics 49:457 

17.
Brünger
AT


Nature
1993
355
527

Brünger AT (1993) Nature 355:527 

18.
Goto
J

Kataoka
R

Hirayama
N


J Med Chem
2004
47
6804
10.1021/jm0493818

15615529


19.
Tronrud
DE


J Appl Cryst
1996
29
100
10.1107/S002188989501421X

Tronrud DE (1996) J Appl Cryst 29:100 

20.
Bostrom
J


J Comput-Aided Mol Design
2002
15
1137
10.1023/A:1015930826903

Bostrom J (2002) J Comput-Aided Mol Design 15:1137 

21.
http://www.rcsb.org


22.
Hooft
RWW

Vriend
G

Sander
C

Abola
EE


Nature
1996
381
272
10.1038/381272a0

8692262


23.
Cruickshank
DW


Acta Cryst
1999
D55
583

Cruickshank DW (1999) Acta Cryst D55:583 

24.
Blow
DM


Acta Cryst
2002
D58
792

Blow DM (2002) Acta Cryst D58:792 

25.
http://www.ccdc.cam.ac.uk/products/life_sciences/validate/astex/pdb_entries


26.
Bursulaya
BD

Totrov
M

Abagyan
R

Brooks
CL


J Comput-Aided Mol Design
2003
17
1
10.1023/B:JCAM.0000017496.76572.6f

Bursulaya BD, Totrov M, Abagyan R, Brooks CL III (2003) J Comput-Aided Mol Design 17:1 

27.
Jones
G

Willett
P

Glen
RC

Leach
AR

Taylor
R


J Mol Biol
1997
267
727
10.1006/jmbi.1996.0897

9126849


28.
Halgren
TA


J Comput Chem
1996
17
490
10.1002/(SICI)1096-987X(199604)17:5/6<490::AID-JCC1>3.0.CO;2-P

Halgren TA (1996) J Comput Chem 17:490 

29.
Perola
E

Charifson
PS


J Med Chem
2004
47
2499
10.1021/jm030563w

15115393


30.
http://www.eyesopen.com/about/events/cup7/stahl/Stahl_cup7_pdb_mistakes.pdf


31.
http://www.farma.ku.dk/index.php/Computational-Chemistry-Ligan/4613/0/


32.
Bostrom J, Grant A (2007) In: Mannhold R (ed) Drug properties: measurement and computation, Chapter 8. Wiley-VCH, Weinheim, Germany

33.
Hartshorn
MJ

Verdonk
ML

Chessari
G

Brewerton
SC

Mooij
WTM

Mortenson
PN

Murray
CW


J Med Chem
2007
50
726
10.1021/jm061277y

17300160


34.
Perola
E

Xu
K

Kollmeyer
TM

Kaufmann
SH

Prendergast
FG

Pang
Y-P


J Med Chem
2000
43
401
10.1021/jm990408a

10669567


35.
Jenkins
JL

Kao
RYT

Shapiro
R


Proteins: Struct Funct Genet
2003
50
81
10.1002/prot.10270

12471601


36.
Forino
M

Jung
D

Easton
JB

Houghton
PJ

Pellechia
M


J Med Chem
2005
48
2278
10.1021/jm048962u

15801821


37.
McGaughey
GB

Sheridan
RP

Bayly
CI

Culberson
JC

Kreatsoulas
C

Lindsley
S

Maiorov
V

Truchon
J-F

Cornell
WD


J Chem Inf Model
2007
47
1504
10.1021/ci700052x

17591764


38.
Seifert
M


J Chem Inf Model
2006
46
1456
10.1021/ci060027n

16711765


39.
Sheridan
RP

Singh
SB

Fluder
EM

Kearsley
SK


J Chem Inf Comput Sci
2001
41
1395
10.1021/ci0100144

11604041


40.
Bursulaya
BD

Totrov
M

Abagyan
R

Brooks
CL


J Comput Aided Mol Des
2003
17
1
10.1023/B:JCAM.0000017496.76572.6f

12926852


41.
Hanley
JA

McNeil
BJ


Radiology
1982
143
29

7063747


42.
Baldi
P


Bioinformatics
2000
16
412
10.1093/bioinformatics/16.5.412

10871264


43.
Tribelleau
N

Acher
F

Brabet
I

Pin
J-P

Bertrand
H-O


J Med Chem
2005
48
2534
10.1021/jm049092j

15801843


44.
Bissantz
C

Folkers
G

Rognan
D


J Med Chem
2000
43
4759
10.1021/jm001044l

11123984


45.
Sills
MA

Weiss
D

Pham
Q

Schweitzer
R

Wu
Y

Wu
JJ


J Biomol Screening
2002
7
191
10.1177/108705710200700304

Sills MA, Weiss D, Pham Q, Schweitzer R, Wu Y, Wu JJ (2002) J Biomol Screening 7:191 

46.
Verdonk
ML

Berdini
V

Hartshorn
MJ

Mooij
WTM

Murray
CW

Taylor
RD

Watson
P


J Chem Inf Comput Sci
2004
44
793
10.1021/ci034289q

15154744


47.
http://www.eyesopen.com


48.
http://www.maybridge.com
. Accessed November 2005

49.
http://www.eyesopen.com


50.
http://www.eyesopen.com/about/events/cup7/hawkins/Hawkins_UKqsar_06_ph.png


51.
Godden
JW

Bajorath
J


J Chem Inf Model
2006
46
1094
10.1021/ci050510i

16711729


52.
Huang
N

Shiochet
BK

Irwin
JJ


J Med Chem
2006
49
6789
10.1021/jm0608356

17154509


53.
Freisner
RA

Banks
JL

Murphy
RB

Halgren
TA

Klicic
JJ

Mainz
DT

Repasky
MP

Knoll
EH

Shelly
M

Perry
JK

Shaw
DE

Francis
P

Shenkin
PS


J Med Chem
2004
47
1739
10.1021/jm0306430

15027865


54.
http://dud.docking.org
. Accessed 5th November 2007

55.
Jain
AJ


J Comput-Aided Mol Des
2007
21
281
10.1007/s10822-007-9114-2

17387436



Abbreviations
AUC
Area under the curve


DPI
Diffraction-coordinate precision index


RMSD
Root mean square deviation


ROC
Receiver operator characteristic




1
Goto et al.’s formula to calculate DPI.

2
18
] coefficient.

3
Enrichment calculation.

http://dx.doi.org/10.1007/s10822-008-9201-z





