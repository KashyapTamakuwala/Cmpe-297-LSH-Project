J Comput Aided Mol Des
Journal of Computer-Aided Molecular Design
0920-654X
1573-4951
Springer Netherlands
Dordrecht


2311385
18338228
9196
10.1007/s10822-008-9196-5
Article


Recommendations for evaluation of computational methods

Jain
Ajay N.

ajain@jainlab.org

1

Nicholls
Anthony

anthony@eyesopen.com

2

1
University of California San Francisco, Box 0128, San Francisco, CA 94143-0128 USA 
2
OpenEye Scientific Software, 9 Bisbee Court, Suite D, Santa Fe, NM 87508 USA 

13
3
2008

3
2008

22
3-4
133
139
5
2
2008

7
2
2008


© Springer Science+Business Media B.V. 2008

The field of computational chemistry, particularly as applied to drug design, has become increasingly important in terms of the practical application of predictive modeling to pharmaceutical research and development. Tools for exploiting protein structures or sets of ligands known to bind particular targets can be used for binding-mode prediction, virtual screening, and prediction of activity. A serious weakness within the field is a lack of standards with respect to quantitative evaluation of methods, data set preparation, and data set sharing. Our goal should be to report new methods or comparative evaluations of methods in a manner that supports decision making for practical applications. Here we propose a modest beginning, with recommendations for requirements on statistical reporting, requirements for data sharing, and best practices for benchmark preparation and usage.

Keywords
Docking
Molecular similarity
Benchmarking
Statistical evaluation

issue-copyright-statement
© Springer Science+Business Media B.V. 2008




Introduction
The field of computational chemistry, particularly as applied to drug design, has become increasingly important in terms of the practical application of predictive modeling to pharmaceutical research and development. Tools for exploiting protein structures or sets of ligands known to bind particular targets can be used for binding-mode prediction, virtual screening, and quantitative prediction of activity. A serious weakness within the field is a lack of standards with respect to statistical evaluation of methods, data set preparation, and data set sharing. Our goal should be to report new methods or comparative evaluations of methods in a manner that supports decision making for practical applications. In this editorial, we propose a modest beginning, with recommendations for requirements on statistical reporting, requirements for data sharing, and best practices for benchmark preparation and usage.
predicting
not
 known at the time that the methods are applied. While this seems elementary, a substantial proportion of recent reports within the field run afoul of this observation in both subtle and unsubtle ways. Rejection of the first premise can reduce scientific reports to advertisements. Rejection (or just misunderstanding) the second premise can distort any conclusions as to practical utility.
1
11
]. The papers collected within this issue make the detailed case for the recommendations that follow; the recommendations are intended to provide guidance to editorial boards and reviewers of work submitted for publication in our field. In surveying the eleven papers, we feel there are three main areas of concern: data sharing, preparation of datasets, and reporting of results. Concerns within each area relate to three main subfields of molecule modeling, i.e. virtual screening, pose prediction, and affinity estimation, and to whether protein structural information is used or not. We describe the issues in each area and then present recommendations drawn from the papers herein.

Data sharing
The issues
precise
7
9
11
].

4
7
9
].

4
7
9
].

4
7
9
].




Recommendations on data sharing
usable
 primary data so that their results may be properly replicated and assessed by independent groups. By usable we mean in routinely parsable formats that include all atomic coordinates for proteins and ligands used as input to the methods subject to study. The commitment to share data should be made at the time of manuscript submission.

publicly available
8
].





Preparation of datasets
The issues
predictions
do not already
know
. For retrospective studies to be of value, the central issue is the relationship between the information available to a method (the input) to the information to be predicted (the output). If knowledge of the input creeps into the output either actively or passively, nominal test results may overestimate performance. Also, if the relationship between input and output in a test data set does not accurately reflect, in character or difficulty, the operational application of the method to be tested, the nominal reported performance might be unrelated to real world performance. Here, we will briefly frame the issue by discussing the differences between the operational use of methods and the construction of tests to measure and document their effectiveness for both protein structure, e.g. docking, and ligand-based methods in their areas of application.

Docking
Pose prediction
Cognate docking
7
7
9
8
9
7
].

Cross docking
7
8
].




Virtual screening
5
8
10
]. One of the frustrations in evaluating a study is how to judge the background against which a method is framed. It is very easy to generate a set of decoys that any method can tell apart from actives, and much more difficult to construct an informative collection.

2
4
5
8
10
]. This is more relevant to ligand-based methods, but also applicable to docking because operationally finding chemically similar molecules as being potentially active is of little value in that these will likely be found by other methods.




Scoring
accurately
Affinity prediction tests can be done absent any affinity data on related analogs. However, to date, successful predictions without prior affinity information have been so anecdotal and untransferable that the field seems willing to accept any input of prior structural information. Hence, inclusion of information as to the protein’s disposition upon binding that is not available in an operational setting is considered acceptable.

3
], such methods are not currently successful when properly considered with control computations that include, for example, correlations of affinity with molecular weight.







Ligand-based modeling
Pose prediction
. This is rarer than the use of ligand information in virtual screening but not operationally uncommon. The goal is to find the alignment of ligands to a protein using one or more known protein–ligand complexes. If the known and predicted ligands are one and the same, then issues from cognate ligand apply, for instance using torsions from the crystal structure, rather than deriving such information. If the known and test ligands are different, then the caveats from cross-docking apply, for instance are the test ligands diverse enough to make this experiment meaningful.

Virtual screening
2
8
11
will not exist
 in libraries to be screened for new leads.

2
].




Scoring
. Predicting affinities of ligands from the affinity of one or more ligands, whether relative or absolute. In practice, we generally have structure-activity data for multiple ligand series. Frequently the problem here is accurately predicting the activity of what would be considered an obvious analog in virtual screening. We do not generally know the bound geometry of the specific ligand whose activity is to be predicted. This methodological area of QSAR has its own set of relatively well-understood foibles and is not addressed in detail in this issue.



The descriptions of test case construction above involve different degrees of challenge in proportion to the amount of information provided to a method. The problems often encountered in reviewing or reading papers is that methods claim a lower level of information concerning the answers than is actually true. This is seldom intentional, no matter the provocation to believe otherwise, but a reflection of the difficulty in preparing a ‘clean’ test.

Recommendations on dataset preparation
Protein structure selection and preparation
4
5
9
free
 are reported. In addition, checking to see if density actually exists for the poses being predicted is suggested, although this requires structure factors to have been deposited along with protein coordinates.

not
5
7
]. At most, selection of sensible protonation states, tautomers, and rotamers of ambiguous or underspecified groups should be done one time for each protein structure. Much fuller disclosure of preparation procedures is required than is typically seen.

other
8
].

4
6
10
11
4
10
11
].




Decoy set construction
4
8
10
11
]. The contributed papers here provide no clear consensus as to what constitutes an acceptable set of decoys, although there are lessons as to what not to do, for instance using molecules that might actually be actives, or have unusual properties compared to known actives. At present, the best suggestions seem to be to make decoys relatively ‘drug-like’, so as to mimic real, i.e. operational screens. We also recommend the practice of employing multiple decoy sets and including those developed by other investigators to facilitate study comparison and collation.

Active ligand set construction
1
2
4
7
2
1
4
6
]. Both ideas seem eminently worth further evaluation.

Ligand preparation
7
9
]. For instance, assign protonation states of ligands and decoys by the same protocol, and generate conformations from just connectivity records of both ligands and decoys.

Parameter tuning
3
4
9
]. There is a dichotomy of opinion on whether “tuned” performance figures are relevant to future application of a method when the correct answer is unknown. Our recommendation is that if one chooses to report tuned performance, one must also report performance using standard parameters.



Even within the constraints outlined above, data set preparation and parameter selection can yield a wide range of results. This is acceptable to illuminate which choices are of most benefit to users of the different methods. However, without strong requirements for data sharing (the subject of the previous section), this benefit will be diluted. Further, without baseline requirements for statistical reporting (the subject of the next section), this diversity will lead to an unacceptable degree of incomparability between different reports.


Reporting results
The issues
what
Pose prediction
8
5
5
7
].

Virtual screening
and
10
1
7
8
10
].

Affinity estimation
3
].

General
10
]. If the test set is not sufficiently different to the training set then there is no assurance against over-parameterized approaches. Finally, reports that profess to predict affinities seldom provide some reliable estimate of experimental affinity. The practice of combining results from multiple experiments is only acceptable if experimental conditions are similar. Anecdotal stories abound of different labs within the same company failing to be able to reproduce each other’s binding affinities, often with difference of an order of magnitude or more. It seems sheer folly to think a test set from truly heterogonous sources can be called reliable.




Recommendations for reporting results
Pose prediction
5
7
]. Statistically it is not impossible in a fair prediction for a measurement to be within, say, 0.1 Å of an experimental measurement that is only accurate to 0.5 Å, but it is unlikely. We also suggest that if an estimate of the precision of the experimental coordinates is available that it must be reported. This, then, provides an excellent bulwark against over-fitting to the known results.

Virtual screening
1
3
7
10
10
].

Affinity estimation
3
8
]. Thirdly, authors must be held responsible for realistic estimates of the accuracy of experimental affinities, in particular when such results are from heterogeneous sources.

General
1
5
7
10
standard metrics
requirement
 and alternates as desired by authors.





Conclusions
Molecular modeling is a relatively young field. As such, its growing pains include the slow development of standards. Our hope for this special issue of JCAMD is that with the help of the arguments made in the contributed papers, the modest recommendations made here will form the kernel of standards that will help us as a community to both improve the methods we develop and to reduce the disparity between reported performance and operational performance.


Acknowledgements
The authors gratefully acknowledge the valuable contributions of Marti Head and Terry Stouch, who participated as discussion leaders during symposium on the “Evaluation of Computational Methods” at the 234th American Chemical Society meeting that led to the development of this special issue. Dr. Jain also acknowledges NIH for partial funding of the work (grant GM070481).

References
1.
Clark RD, Webster-Clark DJ (2008) J Comput Aided Mol Des. doi:10.1007/s10822-008-9181-z

2.
Cleves AE, Jain AN (2007) J Comput Aided Mol Des. doi:10.1007/s10822-007-9150-y

3.
Enyedy IJ, Egan WJ (2007) J Comput Aided Mol Des. doi:10.1007/s10822-007-9165-4

4.
Good AC, Oprea TI (2007) J Comput Aided Mol Des. doi:10.1007/s10822-007-9167-2

5.
Hawkins PCD, Warren GL, Skillman AG, Nicholls A (2007) J Comput Aided Mol Des. doi:10.1007/s10822-007-9166-3

6.
Irwin JJ (2008) J Comput Aided Mol Des. doi:10.1007/s10822-008-9189-4

7.
Jain AN (2007) J Comput Aided Mol Des. doi:10.1007/s10822-007-9151-x

8.
Kirchmair J, Markt P, Distinto S, Wolber G, Langer T (2007) J Comput Aided Mol Des. doi:10.1007/s10822-007-9163-6

9.
Liebeschuetz JW (2008) J Comput Aided Mol Des. doi:10.1007/s10822-008-9169-8

10.
Nicholls A (2008) J Comput Aided Mol Des. doi:10.1007/s10822-008-9170-2

11.
Sheridan RP, McGaughey GB, Cornell WD (2008) J Comput Aided Mol Des. doi:10.1007/s10822-008-9168-9




