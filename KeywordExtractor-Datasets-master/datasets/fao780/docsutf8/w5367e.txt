<title>Quality control of wastewater for irrigated crop production. (Water reports - 10)</title>
<section>1</section>
Preface
It is well recognized that irrigated agriculture can play a key role in finding alternative uses for urban wastewater that is now contaminating many good quality river supplies. A key step is to understand how the wastewater can be safely used. A recent set of international guidelines has been developed by the World Health Organization (WHO) that describes the levels of treatment needed for the various agricultural-type uses. Reality is, however, that treatment facilities sufficient to meet these guidelines may be a decade or more away. As a result, the extent of water supply contamination is increasing and irrigated agriculture is now being assessed as to whether it is linked to the spread of diseases including cholera.
Interim steps need to be taken to control how irrigation water is used. Most of the focus of control has been centred on where direct wastewater irrigation occurs but there is also a need to restrict cropping in areas that have contaminated irrigation supplies. One of the limitations of such an effort is the lack of well defined health-related water quality standards for water actually used for irrigation.
This document reviews the availability of water quality standards and describes an interim approach to standards that emphasizes promotion of safe production areas for high-risk crops such as vegetables. The approach is to assess the quality of water actually being used for irrigation against a known standard. In this interim period, it is proposed to utilize the WHO Guidelines for sewage treatment plant design as irrigation water standards to make this assessment. It is recognized that these guidelines were not intended for such use and there are few, if any, data that link such standards to disease incidence. Considering that the present level of water contamination in many countries already seriously exceeds these guidelines, achieving them in vegetable production areas as an interim step would be a major step toward improving health conditions that result from irrigation practices.
Use of these guidelines is untested. Proposing them here is to begin a process of assessment and eventually working toward a slightly modified version of the guidelines. The approach described here is to utilize these interim guidelines to promote "safe production areas", not to operate a crop restriction programme.
The focus of this report is centred around procedures developed and studied in 1992 in a FAO project in Chile. Because of the concern for the spread of cholera and other diseases by irrigation practices in Chile, both the farmers and the government began an aggressive programme to assess the extent of water supply contamination and to assure the consumer that raw vegetables were being produced in a safe environment. The focus of these joint efforts was "prevention" and that focus is used throughout the document. In addition, the entire programme should be considered interim. When the proper treatment facilities for wastewater are completed such a programme can be abandoned.
It is hoped that this report will attract expert comment. Comments and suggestions for improvement of its practical application to the field would be welcomed and should be addressed to:
Chief, Water Resources, Development and Management Service
Land and Water Development Division
Food and Agriculture Organization of the United Nations
Viale delle Terme di Caracalla
00100 Rome, Italy
<section>2</section>
Acknowledgements
This document could not have been produced without the valuable assistance of the Chilean project team leader, Juan Carlos Cuchacovich, Director of the Department of Natural Resource Protection (DEPROREN), Servicio Agricola y Ganadero (SAG), Ministry of Agriculture, Santiago. The author wishes to express his deep gratitude and appreciation for his contribution to the paper.
Many of the basic data and concepts of the proposed approach in this paper have been developed by numerous professionals. Special appreciation goes to the staffs of WHO, World Bank and FAO who, through their efforts, have raised the awareness of the need to safely manage wastewater in order to gain its resource value. Of special note are Juan Sagardoy, Wulf Klohn and Fernando Chanduvi of the Water Service (AGLW) of FAO who had the vision to see that a start must be made now to control water contamination that is impacting the safety of irrigation water supplies. The author wishes to express his gratitude to Mr Fernando Chanduvi for his thorough review of the text and appropriate incorporation of comments and suggestions from all reviewers. Thanks are also due to Ms. Chrissi Redfern for her assistance in the preparation of the final text.
This document is intended to be one small step in that process. Hopefully the paper will inspire new approaches to an age old problem. The solutions may not, however, be based on the same approaches used before.
<section>3</section>
Chapter 1 - Introduction
The use of domestic wastewater for crop production has been practised for several centuries in one form or another. Prior to the 1940s, most wastewater use occurred on "sewage farms" or areas specifically designated for such use. One of the oldest in the world is the Werribee Farm which serves the City of Melbourne, Australia. This large well-managed farm was established in 1897 and is still in operation today, irrigating some 10 000 ha with wastewater. The impetus for these "sewage farms" was to minimize or prevent pollution in rivers and conserve water and nutrients to improve agriculture (Shuval, 1991). Few of these "sewage farms" still exist today; most were ill-conceived, inadequately funded and poorly regulated, and were eventually abandoned because of public health concerns.
In the mid 1940s, domestic wastewater use again gained increased attention, especially in arid and semi-arid areas that suffer from insufficient overall water supplies. Although the same early motivations for wastewater use remained, the newer areas using wastewater were focused on ensuring they minimized or prevented potential public health problems. The principal concern was use of wastewater on crops normally eaten raw. The change in focus was driven by a better understanding of public health problems and the desire to improve public health standards.
The need to improve public health protection prompted a number of state health departments in the United States to establish guidelines and regulations to control the public health aspects of wastewater use in agriculture. These initial guidelines provided a rational basis for continuing wastewater use by agriculture while meeting strict public health criteria. One important criterion was to restrict the use of partially treated sewage to crops that are generally cooked before being consumed and allow only water that has gone through advanced wastewater treatment and microbial disinfection to be applied to crops normally eaten raw.
Many nations adopted the very strict microbial standards for wastewater use that were developed in California (USA) and elsewhere. In reality these microbial standards were almost unattainable in most wastewater treatment systems, therefore many poorer or developing countries abandoned plans for wastewater use (Shuval et al., 1986a). The primary reason was the realization that producing effluent with a microbial quality sufficient for unrestricted irrigation required costly sophisticated treatment technology. Some of these countries shifted their focus in wastewater use to unrestricted areas of use coupled with crop restrictions. Most, however, did not have a strong institutional structure to control cropping. The result has been little improvement in public health conditions associated with wastewater use. Untreated or partially treated wastewater continues to be used directly for unrestricted irrigation or is discharged to surface water channels where unintended use by
agriculture occurs when water is appropriated for irrigation use.
Over the past 20 years there has been a strong revival of interest in the controlled use of wastewater for crop irrigation. In addition to consumer health protection, the main reasons are:
The last of the above reasons is driven by an increased public awareness of the need for clean water supplies and rivers. This perception, coupled with the population explosion in the urban areas, has resulted in strongly competing demands for water supplies, especially the best quality supplies. Agricultural and rural communities are often left to use the least desirable water supplies including those that have been contaminated with an increasing level of urban wastewater discharges. For example, Mexico is studying the cost-benefit of doubling irrigation with wastewater in the next decade. This would be a means of releasing clean water supplies to cover the domestic needs of nearly 30 million people (Cifuentes et al., 1991/92). This situation is likely to continue in many developing countries until reliable treatment and disposal works are in place. The level of contamination in rivers and irrigation water supplies may be a serious constraint for developing countries as
they strive to produce an adequate and safe food supply in the future.
During the next 20 years while reliable wastewater treatment facilities are planned and constructed, agricultural and water resource planners must face two dilemmas:
There is sufficient information and technology available to plan and execute a wastewater use scheme properly. The key to a successful programme is to control potential public health problems. The differences in approach are centred on where the control or application of public health standards takes place: the point of treatment and discharge; the area where wastewater is used; or the actual point of use for crop production. In reality, the lack of treatment and well defined areas using wastewater in most developing countries will focus control for the near term on the point of use.
The simplest approach is to control the quality of wastewater at its point of treatment and discharge. This places regulation and control at the institutional level as treatment is normally conducted by a public agency. The quality of the discharge can then be regulated to fit the type of use. This alternative assumes that the treatment system is well managed and maintained and produces a reliable quality of effluent. This approach is utilized in the United States, Canada, and Europe and in many cases requires an advanced level of treatment technology. In most developing countries for the present, the lack of treatment works makes this a long-term goal. New approaches to treatment technology in developing countries (Shuval et al., 1986a) will assist in implementing this technology sooner than originally planned, but financial constraints are still likely to make this a long-term effort.
The alternative to controlling the quality of wastewater at its point of treatment and discharge is to control the place where wastewater can be used. This alternative moves around the need for immediate treatment of the wastewater and places an emphasis on controlling where the discharge is used. Under this alternative, wastewater use would be within a defined area and the emphasis would shift to controlling the type of crop production in that area. This approach requires a broader based institutional structure and a strong ability to control cropping in the wastewater use area. The key element is a defined area where cropping restrictions can be practised. This approach is utilized in several developing countries including Tunisia, Mexico, Peru and Kuwait.
The two previous alternatives assume there is either a strict control of wastewater treatment or a well defined area of use. In most developing countries lack of treatment, poorly maintained treatment works, lack of well defined use areas and unrestricted discharges to rivers and canals make using these two alternatives ineffective in the near term. Until treatment works are installed and wastewater use areas defined, most developing countries will be faced with trying to control cropping on a broad scale. Where unrestricted discharges occur to rivers and canals and widespread water appropriation occurs from these water bodies, the dilemma will be whether or not a strong institutional structure is available to implement and enforce cropping restrictions on a broad scale. An alternative approach to crop restrictions is to identify safe production areas and then utilize market pressures to implement a programme that promotes a crop produced in this safe environment.
This document describes an approach that promotes safe production areas. The programme involves evaluating the quality of irrigation water presently used, identifying safe production areas for high-risk crops, such as vegetable crops, and, through a water quality certification programme, promoting the safety of that produce. The goal is to use market pressures to promote safe vegetable products. The programme described here is based upon on an effort in Chile (1992) to control the quality of water used in vegetable production as the irrigation water in Chile was identified as a major mechanism in the spread of cholera and other gastrointestinal diseases (FAO, 1993; Shuval, 1993).
<section>4</section>
Chapter 2 - Health risks associated with wastewater use
Types of pathogens present in wastewater
Pathogens that reach the field or crop
Pathogen survival under agricultural field conditions
Relative health risk from wastewater use
Agronomic conditions that minimize disease spread when wastewater is used for irrigation
Guidelines for public health protection during wastewater use
There are agronomic and economic benefits of wastewater use in agriculture. Irrigation with wastewater can increase the available water supply or release better quality supplies for alternative uses. In addition to these direct economic benefits that conserve natural resources, the fertilizer value of many wastewaters is important. FAO (1992) estimated that typical wastewater effluent from domestic sources could supply all of the nitrogen and much of the phosphorus and potassium that are normally required for agricultural crop production. In addition, micronutrients and organic matter also provide additional benefits.
There are many successful wastewater use schemes throughout the world where nutrient recycling is a major benefit to the project (Pescod and Arar, 1988; FAO, 1992). Rarely, however, is a scheme laid out or planned on the basis of nutrient recycling. The primary constraint to any wastewater use project is public health. Wastewater, especially domestic wastewater, contains pathogens which can cause disease spread when not managed properly. The primary objective of any wastewater use project must therefore be to minimize or eliminate potential health risks.
In most developing countries direct wastewater use projects are normally centred near large metropolitan areas. These schemes often only use a small percentage of the wastewater generated. The result is that indirect use of wastewater prevails inmost developing countries.
Indirect use occurs when treated, partially treated or untreated wastewater is discharged to reservoirs, rivers and canals that supply irrigation water to agriculture. Indirect use poses the same health risks as planned wastewater use projects, but may have a greater potential for health problems because the water user is unaware of the wastewater being present. Indirect use is likely to expand rapidly in the future as urban population growth outstrips the financial resources to build adequate treatment works. Where indirect use occurs, the primary objective must also be to ensure that it is in a manner than minimizes or eliminates potential health risks.
The health hazards associated with direct and indirect wastewater use are of two kinds: the rural health and safety problem for those working on the land or living on or near the land where the water is being used, and the risk that contaminated products from the wastewater use area may subsequently infect humans or animals through consumption or handling of the foodstuff or through secondary human contamination by consuming foodstuffs from animals that used the area (WHO, 1989).
The survival of pathogens and how they infect a new host needs to be understood in developing a programme to eliminate or minimize health risks. The importance and complexity of the rural health problem for those living and working where wastewater is used is beyond the scope of this document. The focus of this document will be on the concern with those who handle, prepare or eat the crop after it has been harvested. The health issues associated with wastewater use for the handlers, preparers and consumers of the crop can be broken down into a series of questions (each will be covered in more detail in subsequent sections of this document):
Types of pathogens present in wastewater
Wastewater or natural water supplies into which wastewater has been discharged, are likely to contain pathogenic organisms similar to those in the original human excreta. Disease prevention programmes have centred upon four groups of pathogens potentially present in such wastes: bacteria, viruses, protozoa and helminths. There have been extensive reviews published on the range of these pathogenic organisms normally found in human excreta and wastewater. The most complete reviews are Feachem et al. (1983), Rose (1986) and Shuval et al. (1986a). The following short discussion is extracted from those reviews and is presented to establish a basic understanding of the pathogens and their abundance.
Bacteria. The faeces of a healthy person contains large numbers of bacteria (> 10^10/g), most of which are not pathogenic. Pathogenic or potentially pathogenic bacteria are normally absent from a healthy intestine unless infection occurs. When infection occurs, large numbers of pathogenic bacteria will be passed in the faeces thus allowing the spread of infection to others. Diarrhoea is the most prevalent type of infection, with cholera the worst form. Typhoid, paratyphoid and other Salmonella type diseases are also caused by bacterial pathogens.
Viruses. Numerous viruses may infect humans and are passed in the faeces (> 10^9/g). Five groups of pathogenic excreted viruses are particularly important: adenoviruses, enteroviruses (including polioviruses), hepatitis A virus, reoviruses and diarrhoea-causing viruses (especially rotavirus).
Protozoa. Many species of protozoa can infect humans and cause diarrhoea and dysentery. Infective forms of these protozoa are often passed as cysts in the faeces and humans are infected when they ingest them. Only three species are considered to be pathogenic: Giardia lamblia, Balantidium coli and Entamoeba histolytica. An asymptomatic carrier state is common in all three and may be responsible for continued transmission.
Helminths. There are many species of parasitic worms or helminths that have human hosts. Some can cause serious illnesses and the ones that pass eggs or larval forms in the excreta are of importance in considering wastewater use. Most helminths do not multiply within the human host, a factor of great importance in understanding their transmission, the ways they cause disease and the effects that environmental change will have on their control. Often the developmental stages (life cycles) through which they pass before reinfecting humans are very complex. Those that have soil, water or plant life as one of their intermediate hosts are extremely important in any scheme where wastewater is used directly or indirectly.
The helminths are classified in two main groups: the roundworms (nematodes) and worms that are flat in cross section. The flatworm, in turn, may be divided into two groups: the tapeworms which form chains of helminths "segments" and the flukes which have a single, flat, unsegmented body. Most of the roundworms that infect humans and also the schistosome flukes have separate sexes. The result is that transmission depends upon infection with both male and female worms and upon meeting, mating and egg production within the human body.
Pathogens that reach the field or crop
All the pathogens discussed in the previous section have the potential to reach the field. From the time of excretion, the potential for all pathogens to cause infection usually declines due to their death or loss of infectivity. The ability of an excreted organism to survive outside the human body is referred to as its persistence. For all the organisms, survival is highly dependent on temperature with greatly increased persistence at lower temperatures.
The first exposure of excreted pathogenic organisms outside the body is usually in water. This blend with freshwater is often referred to as sewage. This sewage is then either subjected to treatment prior to discharge, used directly for crop production or discharged to a watercourse where indirect use then occurs downstream. There are many studies on the survival or persistence of excreted organisms in water and sewage. A summary is shown in Table 1.
Many bacterial populations decline exponentially so that 90 to 99 percent of the bacteria are lost relatively quickly. Survival of bacteria, like many other organisms, depends greatly on how hostile the environment is including other micro-organisms in the water that might provide competition or predation. Bacteria often survive longer in clean water than in dirty water but survival in excess of 50 days is most unlikely and at 20-30°C, 20-30 days is a more common maximum survival time.
Viral survival may be longer than bacterial survival and is greatly increased at lower temperatures. In the 20-30°C range, two months seems a typical survival time, whereas at around 10°C, nine months is a more realistic figure. There is evidence that virus survival is enhanced in polluted waters, presumably as a result of some protective effect that the viruses may receive when they are adsorbed onto suspended solid particles in dirty water.
TABLE 1: Survival times of excreted pathogens in freshwater and sewage at 20-30°C
Pathogen
Survival time (days)
Viruses^a
Enteroviruses^b
<120 but usually <50
Bacteria
Faecal coliform^a
<60 but usually <30
Salmonella spp.^a
<60 but usually <30
Shigella spp.^a
<30 but usually <10
Vibrio cholera^c
<30 but usually <10
Protozoa
Entamoeba histolytica cysts
<30 but usually <15
Helminths
Ascaris lumbriocoides eggs
Many months
a. In seawater, viral survival is less, and bacterial survival is very much less than in freshwater.
b. Includes polio-, echo-, and coxsackieviruses.
c. V. cholera survival in aqueous environments is still uncertain.
Source: Feachem et al. (1983).
Protozoal cysts are poor survivors in any environment. A likely maximum in sewage or polluted water would not exceed that shown in Table 1 for Entamoeba histolytica. Helminth eggs vary from the very fragile to the very persistent. One of the most persistent is the Ascaris egg which may survive for a year or more. The major concern for this helminth is that the soil is its intermediate host prior to reinfecting humans.
The survival times shown in Table 1 may be altered by the type or degree of wastewater treatment given the sewage water prior to use or discharge to a water body. Different treatment processes remove pathogens to varying degrees. What is not well understood in wastewater treatment systems is whether the treatment process produced an elevated level of hostile environment that accelerated the death of the organism or whether the treatment process had little effect on excreted pathogens and simply allowed the necessary time for natural die-off to occur independent of the treatment process.
The critical factor to consider for wastewater use is that most wastewater treatment plants were designed to reduce organic pollution of rivers and lakes and rarely are designed to remove all risks from pathogenic organisms. Therefore, regardless of the level of treatment provided, some pathogenic organisms will reach the agricultural fields when the water is used.
In instances where the sewage water has not received treatment, the level of pathogenic organisms is likely to be higher whether the use is occurring directly from raw sewage or from raw sewage that has been blended with other water supplies. In both instances, pathogenic organisms will reach the agricultural fields. These pathogenic organisms, as with treated sewage, have the potential to contaminate both the soil and the crop depending upon how the irrigation water is used. The critical element is to understand that whether treated, partially treated, or untreated water is used, pathogenic organisms are present and the use site must be managed in a manner that minimizes or eliminates the potential for disease transmission.
TABLE 2: Factors affecting survival time of enteric bacteria in soil
Soil factor
Effect on bacterial survival
Antagonism from soil microflora
Increased survival time in sterile soil
Moisture content
Greater survival time in moist soils and during times of high rainfall
Moisture-holding capacity
Survival time is less in sandy soils than in soils with greater water-holding capacity
Organic matter
Increased survival and possible regrowth when sufficient amounts of organic matter are present
pH
Shorter survival time in acid soils (pH 3-5) than in alkaline soils
Sunlight
Shorter survival time at soil surface
Temperature
Longer survival at low temperatures; longer survival in winter than in summer
Source: Shuval et al. (1986a) as adapted from Gerba et al. (1975).
Pathogen survival under agricultural field conditions
The literature on survival times of excreted pathogens in soil and on crop surfaces has been reviewed by Feachem et al. (1983) and Strauss (1985). As expected there was wide variability in reported survival times which reflects the influence of environmental and analytical factors. Table 2 describes several factors affecting survival time of bacteria in soil. Many of these factors may also affect survival of other pathogenic organisms.
Knowledge of the survival of pathogens in soil and on the crop allows an initial assessment of the risk of transmitting disease via produced foodstuff or through worker exposure. WHO (1989) presented a summary of the potential survival times in agricultural cropping environments (Table 3). WHO concludes that "Available evidence indicates that almost all excreted pathogens can survive in soil... for a sufficient length of time to pose potential risks to farm workers. Pathogens survive on crop surfaces for a shorter time than in the soil as they are less well protected from the harsh effects of sunlight and desiccation. Nevertheless, survival times can be long enough in some cases to pose potential risks to crop handlers and consumers, especially when survival times are longer than crop growing cycles as is often the case with vegetables". While the length of the crop growing cycle is important, equally important is the length of time since the last irrigation cycle (potential
exposure cycle). WHO (1989) points out that excreted pathogens, if they do enter an irrigated area with the irrigation water, have the potential to remain infectious for a considerable period of time thus steps must be taken to interrupt this infection cycle.
Relative health risk from wastewater use
The discussion in the previous sections show that a broad spectrum of pathogenic microorganisms including bacteria, viruses, helminths and protozoa is present in wastewater and they survive for days, weeks and at times months in the soil and on crops that come in contact with wastewater. Early approaches to measuring the health risk from these pathogenic micro-organisms centred on detection. Based upon the fact that these micro-organisms could survive, detection in any of these environments was sufficient to indicate that a public health problem existed. It was then assumed that such detection showed evidence that a real potential for disease transmission existed (Shuval et al., 1986a; Shuval, 1991). This is a "zero-risk" approach. Throughout the years a number of standards and guidelines have been developed on this zero-risk approach. This led to standards for wastewater use that approached those of drinking water especially where vegetable crops were being grown.
TABLE 3: Survival times of selected excreted pathogens in soil and on crop surfaces at 20-30°C
Pathogen
Survival time
In soil
On crops
Viruses
Enteroviruses^a
<100 but usually <20 days
<60 but usually <15 days
Bacteria
Faecal coliform
<70 but usually <20 days
<30 but usually <15 days
Salmonella spp.
<70 but usually <20 days
<30 but usually <15 days
Vibrio cholera
<20 but usually <10 days
<5 but usually <2 days
Protozoa
Entamoeba histolytica cysts
<20 but usually <10 days
<10 but usually < 2 days
Helminths
Ascaris lumbricoides eggs
Many months
<60 but usually <30 days
Hookworm larvae
<90 but usually <30 days
<30 but usually <10 days
Taenia saginata eggs
Many months
<60 but usually <30 days
Trichuris trichiura eggs
Many months
<60 but usually <30 days
^a Includes polio-, echo-, and coxsackieviruses.
Source: WHO (1 989) as summarized from Feachem et al. (1983).
TABLE 4: Effectiveness of enteric pathogens to cause infections through wastewater irrigation related to their epidemiological characteristics
Enteric pathogens
Persistence in environment
Minimum infective dose
Immunity
Concurrent routes of infection
Latency/soil development stage
Viruses
Medium
Low
Long
Mainly home contact and food or water
No
Bacteria
Short/Medium
Medium/High
Short/Medium
Mainly home contact and food or water
No
Protozoa
Short
Low/Medium
None/Little
Mainly home contact and food or water
No
Helminths
Long
Low
None/Little
Mainly soil contact outside home and food
Yes
Source: Shuval et al. (1986b).
Whether a person becomes infected actually depends on a number of additional factors, each of which adds to or diminishes the actual risk of infection. Feachem et al. (1983) and Shuval et al. (1986b) reviewed these factors and found several that are important for determining the relative health risk during wastewater use:
Shuval et al. (1986b) developed a theoretical epidemiological model based on the above factors. The model looked at their relationship to the probability that one of the four enteric pathogen groups described earlier would cause infections in humans through wastewater irrigation. The following factors were considered necessary to cause a high probability of infection:
Table 4 presents the summary of how Shuval et al. (1986b) rated the five factors when considering the enteric pathogen groups.
The Shuval model shows that helminth diseases, if they are endemic, will be very effectively transmitted by irrigation with raw wastewater. On the other hand, the enteric virus diseases should be the least effectively transmitted by irrigation with raw wastewater. The bacterial and protozoan diseases rank between these two extremes. Shuval et al. (1986b) ranked the pathogens in the following descending order of risk:
1. High: Helminths (the intestinal nematodes - Ascaris, Trichuris, hookworm and Taenia)
2. Lower: Bacterial infections (i.e. cholera, typhoid and shigellosis) and Protozoan infections (i.e. ameobiasis, giardiasis)
3. Least: Viral infections (viral gastroenteritis and infectious hepatitis)
This ranking is consistent with the theoretical considerations noted by Feachem et al. (1983) where the determinations were made on factors other than wastewater use. Shuval et al. (1986b) reviewed the available epidemiological evidence to determine whether the theoretical model fitted the empirical evidence. This review concluded that there is evidence of disease transmission in association with the use of raw or partially treated wastewater. This evidence points most strongly to the helminths as the number one problem, particularly in developing countries. There was limited transmission of bacterial and virus disease. The empirical evidence therefore points to the usefulness of the theoretical model and especially the priority ranking for the potential threat of disease transmission. The Shuval model (Table 4) and the rationale behind the ranking of pathogens shown above were reviewed in the World Bank/WHO-sponsored Engelberg Report (IRCWD, 1985) that obtained the
endorsement of an international group of environmental experts and epidemiologists.
Agronomic conditions that minimize disease spread wh