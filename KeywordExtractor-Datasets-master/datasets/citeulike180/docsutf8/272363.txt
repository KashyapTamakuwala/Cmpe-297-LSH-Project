Aaron M. Cohen, MD is a postdoctoral fellow in the medical informatics programme at OHSU. Dr Cohen works in the area of text mining, focusing on issues and applications important to biomedical researchers. He was chairman of the W3C working group that produced version 2 of the Synchronized Multimedia Integration Language (SMIL 2.0). William Hersh, MD is Professor and Chair of the Department of Medical Informatics & Clinical Epidemiology in the School of Medicine at Oregon Health & Science University (OHSU) in Portland, Oregon. Dr Hersh's research focuses on the development and evaluation of information retrieval systems for biomedical practitioners and researchers.

A survey of current work in biomedical text mining
Aaron M. Cohen and William R. Hersh
Date received (in revised form): 25th October 2004

Abstract
The volume of published biomedical research, and therefore the underlying biomedical knowledge base, is expanding at an increasing rate. Among the tools that can aid researchers in coping with this information overload are text mining and knowledge extraction. Significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction and hypothesis generation. Several research groups are constructing integrated flexible text-mining systems intended for multiple uses. The major challenge of biomedical text mining over the next 5­10 years is to make these systems useful to biomedical researchers. This will require enhanced access to full text, better understanding of the feature space of biomedical literature, better methods for measuring the usefulness of systems to users, and continued cooperation with the biomedical research community to ensure that their needs are addressed.

Keywords: text-mining, bioinformatics, natural language processing

INTRODUCTION: BACKGROUND AND PURPOSE
The volume of published biomedical research, and therefore the underlying biomedical knowledge base, is expanding at an increasing rate. While scientific information in general has been growing exponentially for several centuries,1 the absolute numbers specific to modern medicine are very impressive. The MEDLINE 2004 database contains over 12.5 million records, and the database is currently growing at the rate of 500,000 new citations each year.2 With such explosive growth, it is extremely challenging to keep up to date with all of the new discoveries and theories even within one's own field of biomedical research. Biomedical research is divided into highly specialised fields and subfields, with poor communication between disciplines.3 While this may be a necessary pre-condition for the complex and detailed research that biomedical science requires, it also tends to narrow the perspective, impeding the establishment of connections between discoveries

Aaron Michael Cohen, Postdoctoral Fellow, Department of Medical Informatics and Clinical Epidemiology, School of Medicine, Oregon Health & Science University, 3181 S.W. Sam Jackson Park Road, Portland, OR 97239­309, USA Tel: +1 503 494 0046 Fax: +1 503 494 4551 E-mail: cohenaa@ohsu.edu

arising from different research specialties. With the recent sequencing of the human genome, the addition of detailed genetic information to biomedical research makes the situation even more complicated, since genetics may play a role in almost all areas of health and disease and it is likely that many connections between different branches of medicine may be based on related genomic mechanisms. The goal of biomedical research is to discover knowledge and put it to practical use in the forms of diagnosis, prevention and treatment. Clearly with the current rate of growth in published biomedical research, it becomes increasingly likely that important connections between individual elements of biomedical knowledge that could lead toward practical use are not being recognised because there is no individual in a position to make the necessary connections. Methods must be established to aid researchers and physicians in making more efficient use of the existing research and helping them take this research to the next step along the path to practical application. While manual curation and indexing can be an aid to researchers searching for appropriate literature, a

& HENRY STEWART PUBLICATIONS 1467-5463. B R I E F I N G S I N B I O I N F O R M A T I C S . VOL 6. NO 1. 57­71. MARCH 2005

57

Cohen and Hersh

The goal of biomedical text mining is to shift the burden of information overload from the researcher to the computer

Recognising biological entities in text allows for further extraction of relationships and other information by identifying the key concepts of interest

recent study of the information content of MEDLINE records by Kostoff et al.4 found a significant amount of conceptual information present only in the abstract field and missing from the MeSH terms. This is not surprising since the MEDLINE indexers and the MeSH vocabulary, while broadly based, cannot be expected to represent all of the concepts of interest for all potential users. Clearly, the full text of biomedical literature contains a wealth of information important to users that may not be completely captured by reviewers and curators. Text mining and knowledge extraction are ways to aid researchers in coping with information overload. Text mining is differentiated from both information retrieval (IR) and text summarisation (TS) in that while IR and TS focus on the larger units of text such as documents, text mining operates at a finer level of granularity and examines the relationships between specific kinds of information contained both within and between documents. Text mining is also differentiated from full-blown natural language processing (NLP) in that NLP attempts to understand the meaning of text as a whole, while text mining and knowledge extraction concentrate on solving a specific problem in a specific domain identified a priori (possibly using some NLP techniques in the process). For example, text mining can aid database curators by selecting articles most likely to contain information of interest,5,6 or potential new treatments for migraine may be determined by looking for pharmacological substances that are associated with biological processes associated with migraine.7,8 The goal of biomedical text mining is therefore to allow researchers to identify needed information more efficiently, uncover relationships obscured by the sheer volume of available information, and in general shift the burden of information overload from the researcher to the computer by applying algorithmic, statistical and data

management methods to the vast amount of biomedical knowledge that exists in the literature as well as the free text fields of biomedical databases. This paper surveys the state of the art in biomedical text mining over the past 18­ 24 months. The next section covers current active areas of research, including the specific problems that are being addressed and the approaches used. This is followed by an examination of the current issues and future challenges of biomedical text mining.

CURRENT AREAS OF RESEARCH
While other authors have proposed categorisations based on stages of information extraction of increasing sophistication,9 here recent work is grouped pragmatically with separate categories for each distinct type of textmining task. This is because current work centres around several common textmining themes.

Named entity recognition
At first glace, the task of named entity recognition (NER) appears straightforward. The goal is to identify, within a collection of text, all of the instances of a name for a specific type of thing: for example, all of the drug names within a collection of journal articles, or all of the gene names and symbols within a collection of MEDLINE abstracts. Hansich and de Bruijn and coworkers9,10 believed that solving this problem would allow more complex text-mining tasks to be addressed. The idea is that recognising biological entities in text allows for further extraction of relationships and other information by identifying the key concepts of interest and allowing those concepts to be represented in some consistent, normalised form. This task has been challenging for several reasons. First, there does not exist a complete dictionary for most types of biological named entities, so simple textmatching algorithms do not suffice. In addition, the same word or phrase can

58

& HENRY STEWART PUBLICATIONS 1467-5463. B R I E F I N G S I N B I O I N F O R M A T I C S . VOL 6. NO 1. 57­71. MARCH 2005

Current work in biomedical text mining

Approaches to NER generally fall into three categories: lexiconbased, rules-based and statistically-based

refer to a different thing depending upon context (eg ferritin can be a biological substance or a laboratory test). Conversely, many biological entities have several names (eg PTEN and MMAC1 refer the same gene). Biological entities may also have multi-word names (eg carotid artery), so the problem is additionally complicated by the need to determine name boundaries and resolve overlap of candidate names. Because of the potential utility and complexity of the problem, NER has attracted the interest of many researchers, and there is a tremendous amount of published research in this topic. With the large amount of genomic information being generated by biomedical researchers, it should not be surprising that in the genomics era, much of the work in biomedical NER has focused on recognising gene and protein names in free text. The approaches generally fall into three categories: lexicon-based, rules-based and statistically based. Combined approaches also have been used. The output may be a set of tags assigning a predicted type to each word or phrase of interest, as in part-of-speech (POS) tagging,11 or as a score designating the confidence that a word or phrase is of a given type of interest. Systems are typically measured in terms of precision (number of correct predictions divided by total number of predictions) and recall (number of correct predictions divided by number of actual named entities in the text). Precision and recall are often combined into a single measure, either using the F-score, defined as the harmonic mean of precision and recall (2PR/[P+R]),12 or by reporting the balanced precision and recall, defined as the point where precision and recall are equal. One of the most successful rules-based approaches to gene and protein NER in biomedical texts has been the AbGene system of Tanabe and Wilbur.13 It has been used as the NER component in extracting relationships by several other researchers.14,15 AbGene works by

extending the Brill POS tagger11,16,17 to include gene and protein names as a tag type with the system trained on 7,000 hand-tagged sentences from biomedical text. AbGene then applies manually generated post-processing rules based on lexical-statistical characteristics that help further identify the context in which gene names are used and eliminate false positives and negatives. The system achieved a precision of 85.7 per cent at a recall of 66.7 per cent. In contrast to the tagging approach used by Tanabe and Wilbur, Chang et al. created the GAPSCORE system,18 which assigns a numerical score to each word within a sentence by examining the appearance, morphology and context of the word and then applying a classifier trained on these features. Words with higher scores are more likely to be gene and protein names or symbols. After training on the Yapex corpus,19 precision, recall and F-score were computed for both the exact matches and `sloppy' matches (defined as a true positive if any part of gene name is predicted correctly), with the system performing much better with sloppy matches (precision 74 per cent, recall 81 per cent, F-measure 77 per cent), than with exact matches (precision 59 per cent, recall 50 per cent, F-measure 54 per cent). A number of other groups have worked in this area. Hanisch et al. used a large dictionary of gene and protein names and semantically classified words that tend to appear in context with protein names, reporting a specificity of 95 per cent and sensitivity of 90 per cent.10 Zhou et al. trained a hidden Markov model (HMM) on a set of features based on word formation (ie capitalisation), morphology (ie prefix and suffix), POS, semantic triggers (head nouns and verbs) and intra-document name aliases.20 They reported an overall precision of 66.5 per cent at a recall of 66.6 per cent on the GENIA corpus.21 Other gene and protein NER systems include those by Narayanaswamy et al.,22 Settles 23 and Mika and Rost.24

& HENRY STEWART PUBLICATIONS 1467-5463. B R I E F I N G S I N B I O I N F O R M A T I C S . VOL 6. NO 1. 57­71. MARCH 2005

59

Cohen and Hersh

Overall, the performance of state-ofthe-art gene and protein NER systems, achieves F-scores between 75 and 85 percent

Chen and Friedman have adapted the MEDLEE system to recognise phrases that correspond to phenotype information within biomedical text.25 This system uses natural language techniques to identify phenotypic phrases present in journal article abstracts, and recognises phrases containing words separated in the text. This area of biological NER is much less well studied than recognising gene, protein or chemical names, and therefore a smaller knowledge base of phenotypeassociated terms is available. Nevertheless, the investigators were able to automatically import thousands of UMLS terms associated with semantic categories such as cellular body functions and cellular dysfunction, as well as several hundred terms from the Mammalian Ontology. A few hundred other terms were added manually. In a feasibility study of 300 documents, the system achieved a precision of 64.0 per cent with a recall of 77.1 per cent. While, as expected for a new area of study, this performance is lower than that of the gene and protein NER systems, these results were found to be about the same as that of the individual experts used to create the study's gold standard. Overall, the performance of state-ofthe-art gene and protein NER systems achieves F-scores between 75 and 85 per cent. This number is consistent with that found by Hirschman et al. in 2002,12 and the results of Task 1A for the 2004 BioCreative workshop.15 While peak performance does not appear to have increased over the past few years, investigators are obtaining consistent results using a variety of approaches on different data sets. To address this performance plateau and to decrease the computational burden contribution of NER to text mining, Tanabe and Wilbur have used AbGene to generate a large and high-quality gene and protein lexicon of names found in biomedical text.26 The application of AbGene to the MEDLINE database has resulted in an initial collection of over two million putative gene and protein

names. This list was purified by applying thematic analysis to the names, and then using inductive logic programming to learn rules for differentiating gene names from non-gene names within a theme. Finally, a simple false-positive filter was applied that removed obviously incorrect names such as those containing `http' or ending in `tion'. Their approach yielded a final set of 1,145,913 gene names. Assessment of a random sample determined the precision to be approximately 82 per cent. Comparison with a gold standard gave an estimated recall of 61 per cent for exact matches and 88 per cent for partial matches. The quality of this lexicon is about equivalent to the performance of the NER systems, and the large size of the lexicon is a definite advantage. The lexicon could be used with simple or fuzzy matching to efficiently identify gene and protein names as a first step in future text-mining systems. However, the list has been generated with a snapshot of MEDLINE, and given the pace of genomic research, will soon be out of date if it is not updated. Also, the list was built from MEDLINE and not from full text articles, so it is possible that a significant number of gene and protein names exist in the literature and are not found in MEDLINE. It is a subject of current debate how well NER must perform in order to be useful for text mining.9,12 If one assumes that relationship extraction requires identification of three biomedical terms (two entities and one relationship), the performance of relationship extraction should be approximately equal to the cube of the performance of NER. This independence assumption appears to be true for news article extraction. Systems performing named entity extraction on news stories typically perform at an F-score over 90 per cent, and the F-score for new relations is about 75 per cent. For many biomedical applications, the F-score performance rates of relationship mining has often been found to be approximately equal to that of biological

60

& HENRY STEWART PUBLICATIONS 1467-5463. B R I E F I N G S I N B I O I N F O R M A T I C S . VOL 6. NO 1. 57­71. MARCH 2005

Current work in biomedical text mining

The independence assumption does not seem to hold for biological relations

NER, rather than the 60 per cent expected by the independence assumption.27­32 Therefore, the assumption does not seem to hold for biological relations. It may be easier to extract concepts in combination with the relationship between them owing to the increased local context that relationships provide. While some form of NER is useful in most text-mining tasks, the performance level of biological NER is not necessarily rate limiting for other biological text-mining tasks. Nevertheless, we have not reached the point of having standard methods of NER or updated lexicons for biomedical text mining (whatever the asymptotic performance level), so work must continue in this area.

Text classification
Text classification attempts to automatically determine whether a document or part of a document has particular characteristics of interest, usually based on whether the document discusses a given topic or contains a certain type of information. Typically the information of interest is not specified explicitly by the users and, instead, they provide a set of documents that have been found to contain the characteristics of interest (the positive training set), and another set that does not (the negative training set). Text classification systems must automatically extract the features that help determine positives from negatives and apply those features to candidate documents using some kind of decision-making process. Accurate text classification systems can be especially valuable to database curators, who may have to review many documents to find a few that contain the kind of information they are collecting in their database. Because more biomedical information is being created in text form then ever before, and because there are more ongoing database curation efforts to organise this information into coded databases than before, there is a strong need to find useful ways to apply text classification methods to biomedical text.

Accurate text classification systems can be especially valuable to database curators

Yeh et al. ran a text-mining competition as part of the Knowledge Discovery in Databases (KDD) Challenge Cup 2002.6 The task was a curation problem to evaluate papers from the FlyBase data set and determine whether the paper should be curated based on the presence of experimental evidence of Drosophila gene products. The bestperforming entry used a set of manually constructed rules based on POS tagging, a lexicon and semantic constraints determined by examining the training documents.33 The system focused on figure captions, which were found to be useful. An F-score of 78 per cent was achieved on determining whether to curate a paper based on the presence of experimental evidence. Another effective approach looked for manually chosen `keywords' and computed the distance between keywords and gene names.34 Two other well-performing systems used regular expressions to find patterns of words and then used a support vector machine (SVM) to classify the papers.35 In related work, Donaldson et al. used an SVM trained on the words in MEDLINE abstracts to distinguish abstracts containing information on protein­protein interactions, prior to curating this information into their BIND database.36 They used the `bag-of-words' approach with an SVM classifier. A small evaluation with 100 abstracts found a precision of 96 per cent with a recall of 84 per cent. They estimated that the classification system would reduce the number of abstracts that the curators needed to read by about two-thirds. Another investigation in this area used a Probabilistic Latent Categoriser (PLC) with Kullback­Leibler (KL) divergence to re-rank documents returned by PubMed searching for the purposes of curating information into the Swiss-Prot database.37 Evaluation showed a 25­45 per cent precision improvement, with a balanced precision and recall point of about 70 per cent, compared with about 40 per cent for the basic PubMed ranking. Liu et al. performed a unique application

& HENRY STEWART PUBLICATIONS 1467-5463. B R I E F I N G S I N B I O I N F O R M A T I C S . VOL 6. NO 1. 57­71. MARCH 2005

61

Cohen and Hersh

Because of the potential to improve annotator productivity, work on improving biomedical text classification must continue for the foreseeable future

of text classification on figure captions. In a pilot study, they classified the text in figure legends in order to find figures containing representations of protein interactions and signalling events.38 Applying research in text classification to the work processes of actual biomedical curators and annotators is just beginning. The Text Retrieval Conference (TREC) 2004 Genomics Track has a classification problem as one of its tasks.39 The task is meant to mimic the process that the human annotators in the Mouse Genome Informatics (MGI) system go through in order to find documents that contain experimental evidence about genes that they are annotating using Gene Ontology (GO) codes. A full text collection in SGML format has been assembled, realistically reflecting the articles that MGI annotators currently read. In additional, the utility measure used to evaluate performance of the task aims to reflect the priorities of the MGI annotators. Because of the potential to improve annotator productivity, work on improving biomedical text classification to meet the needs of curators and other users must continue for the foreseeable future.

Synonym and abbreviation extraction
Paralleling the growth of the increase in biomedical literature is the growth in biomedical terminology. Because many biomedical entities have multiple names and abbreviations, it would be advantageous to have an automated means to collect these synonyms and abbreviations to aid users doing literature searches. Furthermore, other text-mining tasks could be done more efficiently if all of the synonyms and abbreviations for an entity could be mapped to a single term representing the concept. Most of the work in this type of extraction has focused on uncovering gene name synonyms and biomedical term abbreviations. Several investigators have used gene

name synonym lists created from online databases as a basis for further text mining.36,40,41 However, these gene databases focus on official names and alternates, and are incomplete with respect to the gene names actually found in the literature.42,43 In order to create gene and protein name synonym lists representative of the names used in the literature, Yu and Agichtein44 and Cohen45 have investigated automatic means of extracting gene name synonyms from biomedical free text. Yu and Agichtein applied a combination of four algorithms to full text journal articles. Their system combined the AbGene gene NER system, with statistical, SVM classifier-based, automatic pattern-based and manual rules algorithms. The combined system produced a recall of about 80 per cent with a precision of about 9 per cent, giving an overall F-measure of about 30 per cent. Cohen applied an automatic pattern extraction method to MEDLINE abstracts and a numeric analysis metric on the resulting name co-occurrence network to select the best synonym extraction patterns. While no sophisticated gene NER was used, evaluation showed a precision of 23 per cent, a recall of 21 per cent and an F-score of 22 per cent. The system was also notable for inferring synonyms based on the logical relationship between synonyms found explicitly in the text, increasing recall by about 10 per cent over the same system without inference. Other investigators have applied textmining methods to extracting lists of biomedical abbreviations and their fully specified forms. These methods rely on the proximity of full forms and their abbreviations, and the fact that either the full form or the abbreviation is often enclosed in parentheses. The problem is often reduced to finding the best alignment of the characters in the abbreviation to those in the full form. A variety of alignment and scoring methods have been applied to this basic approach. Liu and Friedman used a large

62

& HENRY STEWART PUBLICATIONS 1467-5463. B R I E F I N G S I N B I O I N F O R M A T I C S . VOL 6. NO 1. 57­71. MARCH 2005

Current work in biomedical text mining

It is thought that, grouping genes by functional relationships could aid gene expression analysis and database annotation

collection of MEDLINE abstracts to determine abbreviations and phrases that were statistically significantly colocated.32 They reported a precision of 96.3 per cent with a recall of 88.5 per cent. Yu et al.46 and Schwartz and Hearst28 applied manually created set of pattern-matching rules to identify abbreviations and their full form. Yu et al. achieved a precision of 95 per cent with 70 per cent recall, while Schwartz and Hearst achieved a precision of 96 per cent at 82 per cent recall for a set of 1,000 MEDLINE abstracts mentioning yeast. Chang et al. trained a logistic regression model with abbreviation specific features and used it to score candidate full forms,47 achieving a precision of 80 per cent with 83 per cent recall on the Medstract corpus.48 The automatic extraction of biomedical abbreviations and their corresponding definition as used within an individual journal article is close to being a solved problem. Research systems uniformly produce high precision and recall. The next step is to integrate these automated extraction capabilities into user systems. For example, an online dictionary of medical abbreviations could be integrated into PubMed to augment search queries. The more general problem of resolving common domain abbreviations undefined in a given journal article is a much more difficult problem dependent on expert knowledge of the specific field and additional, possibly subtle, context from the surrounding text. Gene and protein name synonym extraction has proven to be a more challenging problem. While an automatically updated synonym list would be of great value in augmenting literature searching and text mining, the precision of automatic extraction systems is low enough to introduce an unacceptable level of noise. However, work is being undertaken to standardise the use of official gene and protein names and symbols,43 so this problem may lessen in the future. On the other hand, there will

still be a large legacy of literature that uses non-official names.

Relationship extraction
The goal of relationship extraction is to detect occurrences of a prespecified type of relationship between a pair of entities of given types. While the type of the entities is usually very specific (eg genes, proteins or drugs), the type of relationship may be very general (eg any biochemical association) or very specific (eg a regulatory relationship). Several approaches to extracting relations of interest have been reported in the literature and are applicable to this work. Manually generated template-based methods use patterns (usually in the form of regular expressions) generated by domain experts to extract concepts connected by a specific relation from text.14 Automatic template methods create similar templates automatically by generalising patterns from text surrounding concept pairs known to have the relationship of interest.44,45 Statistical methods identify relationships by looking for concepts that are found with each other more often than would be predicted by chance.7 Finally, NLP-based methods perform a substantial amount of sentence parsing to decompose the text into a structure from which relationships can be readily extracted.31 In the current genomic era, most investigation of this type has centred around relationships between genes and proteins. It is thought that grouping genes by functional relationships could aid gene expression analysis and database annotation.49 Several researchers have investigated the extraction of general relationships between genes. Genes can be grouped or clustered based on how strongly they share words in text containing their names. Raychaudhuri et al. used a measure of neighbour divergence to measure the `functional coherence' of a group of genes.49 They obtained 79 per cent sensitivity at 100 per cent specificity for distinguishing 19 true gene groups from

& HENRY STEWART PUBLICATIONS 1467-5463. B R I E F I N G S I N B I O I N F O R M A T I C S . VOL 6. NO 1. 57­71. MARCH 2005

63

Cohen and Hersh

Currently, the precision and recall obtained for relationship extraction is dependent on the type of relationship to be extracted and literature corpus to be processed

1,900 randomly assembled groups of yeast genes. They later extended their work to include mouse, fly, worm and yeast genes and obtained functional gene groups with 96, 92, 82 and 45 per cent sensitivity at 99.9 per cent specificity.50 Glenisson et al. similarly investigated text-based gene clustering using a vector space approach and the k-medoids algorithm with a cosine similarity metric.51 Wren and Garner identified related genes by analysing the cohesiveness and specificity of the graph structure created by the gene­gene co-occurrences in MEDLINE records.27 They obtained similar results to Raychaudhuri et al. of about 97 per cent specificity at 85 per cent sensitivity. Other research has concentrated extracting specific kinds of relationships between genes, protein, or other biological entities. Gaizauskas et al.'s Protein Active Site Template Acquisition system (PASTA) uses type and POS tagging along with manually created templates and lexicons assembled from biological databases to extract relationships between amino acid residues and their function within a protein.30 Balanced recall and precision was approximately 82 per cent using a manually annotated corpus of MEDLINE abstracts as a gold standard. Albert et al. used dictionaries of protein and interaction terms to identify trioccurrences of two proteins and one interaction within a sentence.41 Applying this approach to the full MEDLINE database looking for interactions between proteins and nuclear receptors they found 3,308 positive interactions, giving a precision of 22 per cent. McDonald et al. combined a hybrid syntactic/semantic grammar in a single parsing process to extract a variety of gene pathway relationships.29 Evaluation using 100 abstracts manually reviewed by a biologist showed 61 per cent precision at 35 per cent recall. Extracting relationships between genes or proteins and GO codes is a task with immediate practical potential that has received much attention lately. The

MeKE system of Chiang and Yu used GO codes as a lexicon of function names, combining it with a lexicon of gene and gene product names from LocusLink, and used a sentence alignment system to determine patterns associated with statements about gene function. They then used the patterns with a Naive Bayes classifier to extract sentences containing information about gene product function.52 Raychaudhuri et al. assigned GO codes by training text classifiers to associate GO codes with abstracts, and then assigning to a gene the strongest maximum entropy associated GO code from the abstracts in which that gene appeared. Evaluation using a subset of yeast genes and GO codes showed that the strongest predicted GO code was accurate about 72 per cent of the time.53 Pan et al.'s Dragon TF association miner system used linear discriminate analysis on terms and neural networks to create models that recognised abstracts th