insight review articles

Computational roles for dopamine in behavioural control
P. Read Montague1,2, Steven E. Hyman3 & Jonathan D. Cohen4,5
Department of Neuroscience and 2Menninger Department of Psychiatry and Behavioral Sciences, Baylor College of Medicine, 1 Baylor Plaza, Houston, Texas 77030, USA (e-mail: read@bcm.tmc.edu) 3 Harvard University, Cambridge, Massachusetts 02138, USA (e-mail: seh@harvard.edu) 4 Department of Psychiatry, University of Pittsburgh and 5Department of Psychology, Center for the Study of Brain, Mind & Behavior, Green Hall, Princeton University, Princeton, New Jersey 08544, USA (e-mail: jdc@princeton.edu)
1

Neuromodulators such as dopamine have a central role in cognitive disorders. In the past decade, biological findings on dopamine function have been infused with concepts taken from computational theories of reinforcement learning. These more abstract approaches have now been applied to describe the biological algorithms at play in our brains when we form value judgements and make choices. The application of such quantitative models has opened up new fields, ripe for attack by young synthesizers and theoreticians.
he concept of behavioural control is intimately tied to the valuation of resources and choices. For example, a creature that moves left instead of right may forgoe the food and other resources that it could have obtained had it chosen right. Such stark, yet simple economic realities select for creatures that evaluate the world quickly and choose appropriate behaviour based on those valuations. From the point of view of selection, the most effective valuations are those that improve reproductive success. This prescription for valuation yields a formula for desires or goals: an organism should desire those things deemed most valuable to it. All mobile organisms possess such discriminatory capacities and can rank numerous dimensions in their world along axes that extend from good to bad. A kind of facile biological wisdom is built into these simple observations and we should expect valuation mechanisms to be built into our nervous systems at every level, from the single neuron to the decision algorithms used in complex social settings. These ideas have recently been upgraded from provocative biological musings to real computational models of how the nervous system sets goals, computes values of particular resources or options, and uses both to guide sequences of behavioural choices. Such models have cast as important players our midbrain's dopamine neurons, whose actions define `rewards' -- our goals or desires -- that should be sought. These neurons have a central role in guiding our behaviour and thoughts. They are hijacked by every addictive drug; they malfunction in mental illness; and they are lost in dramatically impairing illnesses such as Parkinson's disease. If dopamine systems are overstimulated, we may hear voices, experience elaborate bizarre cognitive distortions, or engage excessively in dangerous goal-directed behaviour. Dopamine function is also central to the way that we value our world, including the way that we value money and other human beings. The full story of behavioural control requires vastly more than simple models of dopaminergic function. But here we show how one branch of computational theory -- reinforcement learning -- has informed both the design and interpretation of experiments that probe how the dopamine system influences sequences of choices made about rewards. These models are maturing rapidly and may even guide our understanding of other neuromodulatory systems in the brain, although such applications are still in their infancy.
760
©2004 Nature Publishing Group

T

Reinforcement signals define an agent's goals
Reinforcement learning theories seek to explain how organisms learn to organize their behaviour under the influence of rewards1. `Reward' is an old psychological term defined by Merriam Webster's dictionary as "a stimulus administered to an organism following a correct or desired response that increases the probability of occurrence of the response". Here, we show that current theories of reinforcement learning provide a formal framework for connecting the physiological actions of specific neuromodulatory systems to behavioural control. We focus on dopaminergic systems primarily because they have been most extensively modelled and because they play a major role in decision-making, motor output, executive control and reward-dependent learning2­5. We show how the dopaminergic models provide a way to understand neuroimaging experiments on reward expectancy and cognitive control in human subjects. Finally, we suggest that this same class of model has matured sufficiently for it to be used to address important disturbances in neuromodulation associated with many psychiatric disorders. Despite its name, reinforcement learning is not simply a modern recapitulation of stimulus­response learning, familiar from the classical and instrumental conditioning literature6. Traditional stimulus­response models focused on how direct associations can be learned between stimuli and responses, overlooking the possibility that numerous internal states intervene between the stimulus and its associated response. However, animals clearly have covert internal states that affect overt, measurable behaviour. Reinforcement learning theory explicitly models such intervening states, assumes that some are more desirable than others, and asks how do animals learn to achieve desired states and avoid undesirable ones as efficiently as possible? The answer to this question shows how reinforcement signals define an agent's goals. For simplicity, we focus only on rewards. However, the same story can be told using negative reinforcers (punishments). We refer to the state engendered by a reward as a `goal'. Goals can exist at numerous levels and direct behaviour over many timescales. Goals for humans range from the most basic (for example, procuring something to eat in the next minute) to the most abstract and complex (such as planning a career). In reinforcement learning, it is assumed that the fundamental goal of the agent (learner) is to learn to take actions that are most likely to lead to the greatest accrual of rewards in the future. This goal is achieved under the guidance of simple scalar quantities called reinforcement signals. These signals
NATURE | VOL 431 | 14 OCTOBER 2004 | www.nature.com/nature

insight review articles
reward and predicted reward; instead, it incorporates information about the next prediction made by the reward-prediction system11. In words: current TD error current reward+ ·next prediction current prediction. Here, the words `current' and `next' refer respectively to the present state and to the subsequent state of the learner; is a factor between 0 and 1 that weights the relative influence of the next prediction. By using this reward-prediction error to refine predictions of reward for each state, the system can improve its estimation of the value of each state, and improve its policy function's ability to choose actions that lead to more reward.
The reward-prediction-error hypothesis

Figure 1 TD prediction-error signal encoded in dopamine neuron firing. Electrophysiological recordings from a single dopamine neuron in a monkey during reward-dependent discrimination task. The animal presses a key, two pictures are presented, the animal releases the key and hits the lever under the rewarded picture. If a correct choice is made, juice is delivered after a fixed delay. Juice delivery is marked by vertical bars; neuron spikes by dots. Early on, the juice delivery causes a burst response (top blue arrowhead). This `surprise' response diminishes to zero by the end of learning (bottom blue arrowhead). A `catch trial' using a (surprising) delayed reward time exposes the typical pause (red arrowhead) and burst (top green arrowhead) response. The pause signals that `things are worse than expected' and the burst signals that `things are better than expected'. In the second catch trial, the reward is again surprising, but early rather than late. The burst response for the new delivery time is apparent (lower green arrowhead), but the pause response is less certain (red question mark). Adapted from ref. 13.

Over the past decade, experimental work by Wolfram Schultz and colleagues has shown that dopaminergic neurons of the ventral tegmental area and substantia nigra show phasic changes in spike activity that correlate with the history of reward delivery12­16. It was proposed that these phasic activity changes encode a `prediction error about summed future reward' (as described above): this hypothesis has been tested successfully against a range of physiological data2­3. The `pause' and `burst' responses of dopamine neurons that support a reward-prediction-error hypothesis are shown in Fig. 1. The bursts signal a positive reward-prediction error (`things are better than expected'), and the pauses signal a negative prediction error (`things are worse than expected'). Activity that remains close to the baseline signals that `things are just as expected'. However, this verbal interpretation of dopaminergic activity belies the sophistication of the underlying neural computations1 (Box 1).
Value binding and incentive salience

serve to criticize specific actions or contemplated actions with respect to how effectively they serve the agent's goals. In reinforcement learning, one common goal is the maximization of total future reward6. Every reinforcement learning system possesses three explicitly implemented components: (1) a `reinforcement signal' that assigns a numerical quantity to every state of the agent. Reinforcement signals can be negative or positive. They define the agent's immediate goals by reporting on what is good or bad `right now'; (2) a stored `value function' that formalizes the idea of longer-term judgments by assigning a `value' to the current state of the agent (see Box 1); (3) a `policy function' that maps the agent's states to its actions. Policies are typically stochastic: they assign a probability to each possible action that can be taken from the current state, with the probability weighted by the value of the next state produced by that action. A more concrete description reads as iterations of the following recipe: (1) organism is in state X and receives reward information; (2) organism queries stored value of state X; (3) organism updates stored value of state X based on current reward information; (4) organism selects action based on stored policy; and (5) organism transitions to state Y and receives reward information. In one form of reinforcement learning called temporal-difference learning, a critical signal is the reward-prediction error (also called the temporal-difference, or TD error) 7­9. Unlike the well-known psychological learning rule proposed by Rescorla and Wagner10 in 1972, this error function is not simply a difference between the received
NATURE | VOL 431 | 14 OCTOBER 2004 | www.nature.com/nature

We have presented theoretical evidence that phasic bursts and pauses in midbrain dopaminergic activity are consistent with the formal construct of a reward-prediction error used by reinforcement learning systems (Fig. 1; Box 1). This interpretation is consistent with a long history of physiological and pharmacological data showing that dopamine is involved in appetitive approach behaviour17­19, and is a key component in the pathologies of behavioural control associated with drug addiction20­21. One finding offered as a challenge to the models discussed so far is that antagonism of dopamine receptors does not change the appetitive value of food rewards but does prevent the treated animal from initiating actions that allow it to obtain the food reward17,22 . In these experiments, animals treated with dopamine-receptor blockers are virtually unable to link sequences of actions to obtain a food reward, but they will consume the same amount as untreated animals if they are moved close to the food rewards by the experimenter (Fig. 2). This conclusion also holds for the inhibition of dopamine neuron firing by gamma-aminobutyric acid (GABA) injected directly into the ventral tegmental area (Fig. 2). These data suggest that interfering with dopamine transmission does not alter the internal evaluation of rewards, but simply the ability to act on those valuations. Addressing these data at a conceptual level, Berridge and Robinson have proposed that dopamine mediates the `binding' between the hedonic evaluation of stimuli and the assignment of these values to objects or acts17. They call this idea `incentive salience'. Although competing psychological explanations differ with respect to the specific claims of incentive salience19,23,24, they all agree that dopamine release and binding is a necessary link between the evaluation of potential future rewards and the policy (sequence of actions) that acquires the rewards. Here, we refer to this link as value binding and distinguish three components: (1) the value computation; (2) the link to a policy (value binding); and (3) execution of the policy.
Incentive salience and actor­critic models

There is a class of reinforcement learning model, called the actor­critic that is closely related to the Berridge and Robinson model for the role of dopamine in value and action learning1,9. In these models, the `critic' carries the reward-prediction error associated
761

©2004 Nature Publishing Group

insight review articles
Box 1 Value functions and prediction errors
The value function In the simplest TD models of dopamine systems, the reward-prediction error depends on a value function that equates the value V of the current state s at time t with the average sum of future rewards received up until the end of a learning trial. V(st) average sum of future rewards delivered from state st until the end of a learning trial average [rt+rt+1+rt+2+ ...+r (trial's end)] E r( )
Trial

the value of its successor state s t + 1. Until this point, we have been discussing the ideal case for V. However, as indicated above, V ^ cannot be known exactly in the real world. Instead, an estimate V of V must be formed within the nervous system. The TD algorithm learns ^ an approximation V of the value function V. It uses a prediction-error signal: (t) ^ ^ prediction error (t) E[rt]+V (st+1) V (st) current reward+next prediction current prediction

(3)

(1) This TD error signal reproduces the phasic burst and pause responses measured in dopamine neurons recorded in alert monkeys during learning tasks. The next value of each adaptable weight w(t+1) used to estimate V is incremented or decremented in proportion to the product of the current prediction error (t) and the current representation s(t) of the stimulus responsible for the prediction. w(t+1) w(t)+ s(t) (t) (4)

E is the expected value operator. There are two sources of randomness over which the above averaging occurs. First, the rewards in a trial [rt+rt+1+rt+2+...+r (trial's end)] are random variables indexed by the time t. For example, rt+2 is a sample of the distribution of rewards received two timesteps into the trial. The idea is that the animal can learn the average value of these rewards by repeating learning trials, and by revisiting state st sufficiently frequently for its nervous system to be able to estimate the average value of each of the rewards received from state st until the end of the trial. The second source of randomness is the probabilistic transition from one state at time t to a succeeding state s t+1 at a later time t+1. The value function, stored within the nervous system of the creature, provides an assessment of the likely future rewards for each state of the creature; that is, the value must somehow be associated with the state. However, as written in equation (1), it would be virtually impossible to make good estimates of the ideal V(st) as it is now defined. This is because the creature would have to wait until all rewards were received within a trial before deciding on the value of its state at the beginning of the trial. By that time, it is too late for such a computation to be useful. This problem becomes worse in real-world settings. Fortunately, equation (1) provides a way out of this dilemma because it obeys a recursion relation through time: V(st) E[r t ]+V(s t+1) (2)

Here, is a learning rate. Exponential discounting of future rewards The artificial truncation at the end of a trial (equation (1)) can be handled theoretically in several ways. One popular formalization is to weight the near future more than the distant future. In this case, the analogue to equation (1) takes the form: Vd ( (t)) average sum of discounted future rewards average [ 0r(t)+ 1r(t+1)+ 2r(t+2)+...] for 0< <1 E
t
t

r( )

Using this weighted version of the value function, the learning episodes for a creature do not have to be artificially divided into `trials'. An analogous reward-prediction-error signal can be formed and used in the same manner as above: (t) ^ ^ prediction error (t) E[rt]+ V (st+1) V (st) current reward+ next prediction current prediction

This recursion relation shows that information about the value of a state s t is available using only the value V(st) of the current state s t and

(5)

a

Value binding Dopamine required to bind value to action Start Intake Running Activity

b

Actor­critic models Value function Reward function

 V(s(t+1)) ­ V (s(t ))

r (t)

Goal 100 Control (%) 80 60 40 20 Block dopamine binding in NAc

+
Critic  (t)

Dopamine neurons (VTA)

Actor Inhibit dopamine neuron spiking in VTA

 wi =  (t)
Reward learning

P = (1 + e ­ (m (t) + b))­1 Action selection

Figure 2 Equating incentive salience with the actor­critic model. a, Rats are trained to run a maze to acquire sugary water. If dopaminergic spiking is blocked (left histograms) in the VTA, then rats will generally not run down the maze to get a reward and are less active. However, if the experimenter moves them to the sugary water, the rats drink exactly the same amount as untreated rats. This suggests that the (hedonic) value of the sugary water has been computed but that the capacity to

bind this value to actions required to obtain the water fails to function. The same effect results if dopamine's interaction with its receptor is blocked in an important downstream target of dopamine projections (right histograms). Adapted from refs 22 and 25. b, Actor­critic models use dopamine-encoded prediction-error signal in two roles: (1) to learn stimulus­reward associations, and (2) to assess actions or contemplated actions (notations are as in Box 1). Adapted from refs 2, 25, 83.
NATURE | VOL 431 | 14 OCTOBER 2004 | www.nature.com/nature

762
©2004 Nature Publishing Group

insight review articles
$5 anticipated $5 outcome 0.2 $1 anticipated $1 outcome $0.2 anticipated $0.2 outcome 0.1

0.0

­0.1 2 4 6 8 Time (s) 10 12 14

Figure 3 Scaled responses to a monetary reward in the ventral striatum. Action is required to receive a reward. The haemodynamic response is modulated by the amount of money received. In both cases, positive deviations in expectations make the responses bigger. Adapted from ref. 38.

tegmental area (VTA), and hypothalamus (Hyp). All these regions have topographically organized reciprocal connections with the VTA -- one of the primary dopaminergic nuclei in the brainstem. Particularly strong reward responses have been observed in the ventral striatum where numerous studies have shown that even abstract proxies for reward (money) cause activations that scale in proportion to reward amount or deviation from an expected payoff 37­39. Similar results have been found by a variety of groups using both passive and active games with monetary payoffs40­42 . A prominent activation response to monetary payoff was observed by Knutson and colleagues in the NAc and is shown in Fig. 3. The NAc, like the OFC and other parts of the prefrontal cortex (PFC), is densely innervated by dopaminergic fibres originating from neurons housed in the midbrain. Other work has shown that simply changing the predictability of a stimulus will activate the NAc and surrounding structures in the ventral parts of the striatum30. The picture emerging from this work is that responses in this region may reflect an encoding of rewards along a common valuation scale43.
Human critic responses

Signal change (%)

with the states of the organism. The `actor' uses this signal, or a closely related one, to learn stimulus­action associations, so that actions associated with higher rewards are more likely to be chosen. Together, these two components capture many features of the way that animals learn basic contingencies between their actions and the rewards associated with those actions. The original hypothesis concerning the role of dopamine in reinforcement learning proposed just such a dual use of the reward-prediction-error signal2,25. McClure and colleagues recently extended this original learning hypothesis to address the Berridge and Robinson model26. This work suggests a formal relationship between the incentive-salience ideas of Berridge and Robinson and actor­critic models in which incentive salience is equivalent to the idea of expected future value formalized in reinforcement learning models (Fig. 2). Actor­critic models are now being used to address detailed issues concerning stimulus­action learning8. For example, extensions to actor­critic models have addressed the difference between learning goal-directed approach behaviour and learning automatic actions (habits), such as licking. There are several behavioural settings that support the contention that habit learning is handled by different neural systems from those involved in goal-directed learning27,28. Dayan and Balleine have recently offered a computational extension to actor­critic models to take account of this fact29.

One of the most important contributions of reinforcement learning theory has been to distinguish between the signalling of the reward itself, and the computation of the reward-prediction error. Using passive tasks with a juice reward, reward-prediction errors have been shown to activate structures in the ventral striatum30,44. Recently, two independent groups used passive learning paradigms to visualize reward-prediction-error signals in overlapping regions of the ventral putamen32,33 (Fig. 4). The cingulate cortex is another area that has been associated with reinforcement learning signals that seem to be reward-prediction errors. The error-related negativity (ERN) is a scalp-recorded event-related potential (ERP), believed to originate from the anterior cingulate cortex, that is consistently observed about 100 msec following the commission of an error45,46. Similar potentials have been observed following negative feedback or unexpected losses in gambling tasks47­49. Holroyd and Coles have proposed that these potentials reflect a negative reward-prediction-error signal, and this idea has been tested under a variety of conditions50­52. Recently, fMRI evidence has suggested that a region of anterior cingulate cortex responds under many of the same conditions as the ERN: activity is affected by both errors and negative feedback53.
Human actor responses

Rewards, critics and actors in the human brain
Recent functional magnetic resonance imaging (fMRI) experiments have used reward expectancy and conditioning tasks to identify brain responses that correlate directly with rewards, reward-predictionerror signals (critic), and signals related to reward-dependent actions (actor). Many of these experiments have used reinforcement learning models as a way to understand the resulting brain responses, to choose design details of the experiment, or to locate brain responses associated with specific model components30­34.
Human reward responses

One implication of reinforcement theory for behaviour concerns the relationship between reward-prediction errors (critic signals) and action selection (actor signals). As discussed in Box 1, the critic signal can be used for reward learning and to adjust the future selection of reward-yielding actions. Success in the use of fMRI to detect rewardprediction-error signals inspired O'Doherty and colleagues to carry out a clever, but simple experiment designed to relate critic signals to action selection34. The experiment used a conditioning paradigm that was carried out in two modes. The first required an action to obtain a juice reward and the second did not. This experiment showed that activity in the dorsal striatum correlated with the predictionerror signal only when an action was needed to acquire the juice reward (Fig. 4c). There was no similar activity in this area when the juice was passively delivered. This finding is important because the dorsal striatum is involved in the selection and sequencing of actions.

Responses to rewarding stimuli have been observed consistently from the same set of subcortical regions in human brains, suggesting that neurons in these regions respond to a wide spectrum of triggers. In a series of elegant papers, Breiter and colleagues used fMRI to record brain responses to beautiful visual images35 and drugs that induce euphoria (cocaine)36. The brain structures they identified included the orbitofrontal cortex (OFC), amygdala (Amyg), nucleus accumbens (NAc; part of the ventral striatum), sublenticular extended amygdala (SLEA; part of the basal forebrain), ventral
NATURE | VOL 431 | 14 OCTOBER 2004 | www.nature.com/nature

Neuromodulation and cognitive control
Our consideration of reinforcement learning theory so far has focused on simple situations, involving the association of a stimulus with a reward, or with the selection of an action that leads to an immediate reward. In the real world, however, accrual of reward may require an extended sequence of actions. Furthermore, we have considered only a highly abstracted definition of the goal of the organism -- the maximization of cumulative future rewards. However, many different forms of reward (and associated actions) may be
763

©2004 Nature Publishing Group

insight review articles
valued by an organism (for example, the procurement of nutrition, provision of safety, reproduction). This suggests that the construct of a goal needs to be refined to describe the variety of goal-directed behaviour in which humans engage. The guidance of behaviour in the service of internally represented goals or intentions, is often referred to as the capacity for cognitive control. Recent theories of cognitive control have elaborated on basic reinforcement learning mechanisms to develop models that specifically address the two challenges suggested above: (1) the need to learn and control sequences of actions required to achieve a goal; and (2) the need to represent the variety of goals that an organism may value. Here, we focus on the first of these challenges, but seerefs 54 and 55 for a discussion of the latter.
Prefrontal goals

require an extended sequence of actions. Theories of cognitive control consistently implicate the PFC as a site where representations of goals are actively maintained and used to select goal-directed behaviours54. The involvement of the PFC is motivated by three diverse classes of observations: (1) the PFC can support sustained activity in the face of distracting information56,57; (2) damage to the PFC produces deficits in goal-directed behaviour58,59; and (3) the PFC is selectively engaged by tasks that rely heavily on the active representation of goal information60.
Dopamine gating hypothesis

Pursuit of a goal (for example, going to the car, driving to the grocery store, or locating the refrigerated section to buy milk), can often
a
Right

R

b
0.8 Signal change (%) 0.4 0.0 ­0.4 2 6 Ventral putamen

*
+ ­

One problem with the simple hypothesis that the PFC actively maintains goal representations is that this does not indicate how or when this information should be updated. Failure to appropriately update goal representations will lead to perseverative behaviour, whereas failure to adequately maintain them will result in distractability. Indeed, disturbances of the PFC are known to be associated with distractability, perseveration, or both61. What is required is a mechanism that can signal when the goal representation should be updated. Recently, it has been proposed that dopaminergic signals from the VTA implement this mechanism, by controlling the `gating' of afferent information into the PFC 55,62 (Fig. 5). According to this gating hypothesis, the PFC is resistant to the influence of afferent signals in the absence of phasic dopamine release, allowing it to preserve the currently maintained goal representation against impinging sources of interference. However, stimuli that signal the need to update the goal representation elicit a phasic dopamine response that `opens the gate' and allows afferent signals to establish a new goal representation in the PFC.
Reinforcement learning and working memory

*
How does the dopamine system know which stimuli should elicit a gating signal and which should not? One plausible answer to this question comes directly from the reinforcement learning theory of dopamine function. A gating signal is required to update the PFC when a stimulus occurs in the environment which indicates that a more valuable goal can be achieved if behaviour is redirected towards that goal (for example, a light signalling that a reward can be acquired by going to some new location). In reinforcement learning terms, this corresponds to a positive reward-prediction error: the value of the current state is better than expected. According to the reinforcement learning theory of dopamine function, this is associated with a phasic burst in dopamine activity. In other words, reinforcement learning theory predicts that phasic dopamine responses will occur precisely when needed to produce a gating signal. Furthermore, insofar as the phasic dopamine response acts as a learning signal, it will strengthen the association of the current predictor, for example, the light, with the goal representation in the PFC. It will also strengthen the tendency of the light to elicit a phasic dopamine response when it recurs in the future. The learning here is analogous to the simple `light-predicts-juice' experiments described earlier, except that now `light predicts goal representation in the PFC', which in turn leads to the accrual of reward. This proposal shows how a prefrontal representation that plays a causal role in the acquisition of some later reward comes to be selected and reinforced. Assuming that dopamine generates both learning and gating effects, the dopamine system provides a mechanism for learning which stimuli should elicit a gating signal to update goal representations in the PFC. Consistent with this hypothesis, the parameter used to implement the learning effects of dopamine in formal models of reinforcement learning2,8,30,63 bears a remarkable similarity to the parameter used to implement gating effects in models of dopamine-based gating signals in the PFC63. Recent computational modelling work has demonstrated that implementing concurrent effects of dopamine phasic signals on reinforcement learning and gating allows a system to associate stimuli with the gating signals that predict reward, and so learn how to update representations appropriately in the PFC62,64,65 .
NATURE | VOL 431 | 14 OCTOBER 2004 | www.nature.com/nature

*
10 Time (s) 12

c

Action required
Right

No action required

Figure 4 Detecting actor and critic signals in the human brain using fMRI. a, A simple conditioning task reveals a TD-like prediction-error response (critic signal) in the human brain. A cue is followed by the passive delivery of pleasant-tasting juice while subjects are scanned. The highlighted activation is located in the ventral part of the striatum (the putamen) -- a region known to respond to a range of rewards. The activation represents the brain response that correlates with a continuous TDlike error signal. Adapted from ref. 30. b, A similar experimental design, but in this case a single prediction error of each polarity (positive and negative) can be seen in the ventral putamen during a surprising catch trial. Predictive sensory cue (green arrowhead); normal reward-delivery time (blue arrowhead); delayed reward time on catch trials (red arrowhead). Average BOLD (blood oxygenation level dependent) response in normal trials (solid line) and delay trials (dashed lin